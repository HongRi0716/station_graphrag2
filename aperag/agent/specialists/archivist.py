import logging
from typing import Any, Dict, List, Optional
from datetime import datetime

from aperag.agent.core.base import BaseAgent
from aperag.agent.core.models import AgentRole, AgentState

logger = logging.getLogger(__name__)


class ArchivistAgent(BaseAgent):
    """
    å›¾è°±ä¸“å®¶ (The Archivist)
    
    èŒè´£ï¼š
    - çŸ¥è¯†åº“æ£€ç´¢
    - å›¾è°±å…³ç³»éå†
    - å†å²æ•°æ®æŸ¥è¯¢
    - çŸ¥è¯†æ•´åˆ
    """

    def __init__(self, retrieve_service: Any = None):
        super().__init__(
            role=AgentRole.ARCHIVIST,
            name="å›¾è°±ä¸“å®¶ (Archivist)",
            description="æ‹¥æœ‰å…¨å±€çŸ¥è¯†åº“çš„è®¿é—®æƒé™ï¼Œæ“…é•¿æŸ¥æ‰¾è®¾å¤‡å°è´¦ã€å†å²ç¼ºé™·è®°å½•ã€æ£€ä¿®è§„ç¨‹å’ŒæŠ€æœ¯æ–‡æ¡£ã€‚",
            tools=["global_search", "graph_traversal", "rag"],
        )
        self.retrieve_service = retrieve_service

    async def _execute(self, state: AgentState, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæ£€ç´¢ä»»åŠ¡"""
        query = input_data.get("query", input_data.get("task", ""))
        search_type = input_data.get("search_type", "hybrid")  # vector, graph, hybrid
        
        collection_ids = input_data.get("collection_ids")
        
        self._log_thought(state, "thought", f"å›¾è°±ä¸“å®¶æ¥æ”¶æŸ¥è¯¢: {query}")
        
        # åˆ¤æ–­æŸ¥è¯¢ç±»å‹
        if any(keyword in query for keyword in ["å…³ç³»", "è¿æ¥", "è·¯å¾„", "å…³è”"]):
            return await self._graph_traversal(state, query)
        elif any(keyword in query for keyword in ["å†å²", "æ¡ˆä¾‹", "è®°å½•"]):
            return await self._historical_search(state, query, collection_ids)
        else:
            return await self._knowledge_search(state, query, search_type, collection_ids)
    
    async def _knowledge_search(
        self,
        state: AgentState,
        query: str,
        search_type: str,
        collection_ids: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """çŸ¥è¯†åº“æ£€ç´¢"""
        self._log_thought(state, "action", f"æ‰§è¡Œ{search_type}æ£€ç´¢")
        
        if self.user_id:
            try:
                # å¦‚æœæœªæŒ‡å®šçŸ¥è¯†åº“ï¼Œåˆ™è·å–ç”¨æˆ·çš„æ‰€æœ‰çŸ¥è¯†åº“ï¼ˆå›¾è°±ä¸“å®¶é»˜è®¤æœç´¢å…¨å±€ï¼‰
                if not collection_ids:
                    from aperag.service.collection_service import collection_service
                    collections = await collection_service.get_all_collections(self.user_id)
                    collection_ids = [str(c.id) for c in collections]
                    
                    if not collection_ids:
                        self._log_thought(state, "observation", "ç”¨æˆ·æ²¡æœ‰çŸ¥è¯†åº“ï¼Œè·³è¿‡æ£€ç´¢")
                        return self._fallback_response(query)

                # ä½¿ç”¨BaseAgentçš„æ£€ç´¢èƒ½åŠ›
                results = await self._search_knowledge(
                    state=state,
                    query=query,
                    collection_ids=collection_ids,
                    top_k=10
                )
                
                # æå–æ–‡æ¡£
                documents = self._extract_documents_from_tool_results(results)
                
                self._log_thought(
                    state,
                    "observation",
                    f"æ£€ç´¢åˆ° {len(documents)} æ¡ç›¸å…³æ–‡æ¡£"
                )
                
                # æ„å»ºç»“æœæŠ¥å‘Š
                report = self._format_search_results(query, documents)
                
                return {
                    "answer": report,
                    "content": report,
                    "documents": documents,
                    "source_documents": documents,
                    "count": len(documents)
                }
                
            except Exception as e:
                logger.warning(f"Knowledge search failed: {e}")
                self._log_thought(state, "correction", f"æ£€ç´¢å¤±è´¥: {str(e)}")
                return self._fallback_response(query)
        else:
            # æ²¡æœ‰user_idï¼Œä½¿ç”¨Mockæ•°æ®
            return self._fallback_response(query)
    
    async def _graph_traversal(
        self,
        state: AgentState,
        query: str
    ) -> Dict[str, Any]:
        """å›¾è°±å…³ç³»éå†"""
        self._log_thought(state, "action", "æ‰§è¡Œå›¾è°±éå†")
        
        if self.user_id:
            try:
                # ä½¿ç”¨LLMåˆ†ææŸ¥è¯¢æ„å›¾
                intent_prompt = f"""
åˆ†æä»¥ä¸‹æŸ¥è¯¢çš„å›¾è°±éå†éœ€æ±‚ï¼š
æŸ¥è¯¢: {query}

è¯·æå–ï¼š
1. èµ·å§‹èŠ‚ç‚¹ï¼ˆè®¾å¤‡åç§°ï¼‰
2. ç›®æ ‡èŠ‚ç‚¹ï¼ˆå¦‚æœæœ‰ï¼‰
3. å…³ç³»ç±»å‹ï¼ˆå¦‚ï¼šè¿æ¥ã€ä¾›ç”µã€ä¿æŠ¤ç­‰ï¼‰
4. éå†æ·±åº¦ï¼ˆ1-3ï¼‰

ä»¥JSONæ ¼å¼è¾“å‡ºï¼š
{{
    "start_node": "èµ·å§‹èŠ‚ç‚¹",
    "target_node": "ç›®æ ‡èŠ‚ç‚¹æˆ–null",
    "relation_type": "å…³ç³»ç±»å‹",
    "depth": 2
}}

åªè¾“å‡ºJSONï¼Œä¸è¦å…¶ä»–è¯´æ˜ã€‚
"""
                
                self._log_thought(state, "action", "ä½¿ç”¨LLMåˆ†ææŸ¥è¯¢æ„å›¾")
                
                intent_json = await self._generate_with_llm(
                    state=state,
                    prompt=intent_prompt,
                    temperature=0.3,
                    max_tokens=500
                )
                
                import json
                import re
                
                cleaned_intent = re.sub(r"```json|```", "", intent_json).strip()
                intent = json.loads(cleaned_intent)
                
                self._log_thought(
                    state,
                    "observation",
                    f"è¯†åˆ«æ„å›¾: {intent}"
                )
                
                # æ‰§è¡Œå›¾è°±éå†ï¼ˆè°ƒç”¨å›¾è°±å·¥å…·ï¼‰
                # è¿™é‡Œæˆ‘ä»¬æ¨¡æ‹Ÿä¸€ä¸ªå›¾è°±æŸ¥è¯¢ï¼Œå¹¶è¦æ±‚LLMè¿”å›ç»“æ„åŒ–æ•°æ®
                traversal_prompt = f"""
è¯·æŸ¥è¯¢çŸ¥è¯†å›¾è°±ä¸­çš„å…³ç³»ï¼š
èµ·å§‹èŠ‚ç‚¹: {intent.get('start_node')}
ç›®æ ‡èŠ‚ç‚¹: {intent.get('target_node', 'æ‰€æœ‰ç›¸å…³èŠ‚ç‚¹')}
å…³ç³»ç±»å‹: {intent.get('relation_type', 'æ‰€æœ‰å…³ç³»')}
éå†æ·±åº¦: {intent.get('depth', 2)}

è¯·è¿”å›ä¸¤éƒ¨åˆ†å†…å®¹ï¼š
1. è‡ªç„¶è¯­è¨€æè¿°ï¼šæè¿°æ‰¾åˆ°çš„å…³ç³»å’Œè·¯å¾„ã€‚
2. ç»“æ„åŒ–æ•°æ®ï¼šä»¥JSONæ ¼å¼åˆ—å‡ºæ¶‰åŠçš„èŠ‚ç‚¹å’Œè¾¹ã€‚

æ ¼å¼è¦æ±‚ï¼š
[DESCRIPTION]
...è‡ªç„¶è¯­è¨€æè¿°...

[GRAPH_DATA]
{{
    "nodes": [
        {{"id": "èŠ‚ç‚¹ID", "label": "èŠ‚ç‚¹åç§°", "type": "è®¾å¤‡ç±»å‹"}}
    ],
    "edges": [
        {{"source": "æºèŠ‚ç‚¹ID", "target": "ç›®æ ‡èŠ‚ç‚¹ID", "label": "å…³ç³»ç±»å‹"}}
    ]
}}
"""
                
                traversal_result_raw = await self._generate_with_llm(
                    state=state,
                    prompt=traversal_prompt,
                    temperature=0.5
                )
                
                # è§£æç»“æœ
                description = traversal_result_raw
                graph_data = {"nodes": [], "edges": []}
                
                if "[GRAPH_DATA]" in traversal_result_raw:
                    parts = traversal_result_raw.split("[GRAPH_DATA]")
                    description = parts[0].replace("[DESCRIPTION]", "").strip()
                    try:
                        graph_json_str = parts[1].strip()
                        graph_json_str = re.sub(r"```json|```", "", graph_json_str).strip()
                        graph_data = json.loads(graph_json_str)
                    except Exception as e:
                        logger.warning(f"Failed to parse graph data: {e}")
                
                return {
                    "answer": description,
                    "content": description,
                    "intent": intent,
                    "graph_data": graph_data
                }
                
            except Exception as e:
                logger.warning(f"Graph traversal failed: {e}")
                self._log_thought(state, "correction", f"å›¾è°±éå†å¤±è´¥: {str(e)}")
                return self._fallback_response(query)
        else:
            return self._fallback_response(query)
    
    async def _historical_search(
        self,
        state: AgentState,
        query: str,
        collection_ids: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """å†å²æ•°æ®æŸ¥è¯¢"""
        self._log_thought(state, "action", "æ£€ç´¢å†å²æ•°æ®")
        
        # æ£€ç´¢å†å²è®°å½•
        if self.user_id:
            try:
                # å¦‚æœæœªæŒ‡å®šçŸ¥è¯†åº“ï¼Œåˆ™è·å–ç”¨æˆ·çš„æ‰€æœ‰çŸ¥è¯†åº“
                if not collection_ids:
                    from aperag.service.collection_service import collection_service
                    collections = await collection_service.get_all_collections(self.user_id)
                    collection_ids = [str(c.id) for c in collections]

                results = await self._search_knowledge(
                    state=state,
                    query=query,
                    collection_ids=collection_ids,
                    top_k=20  # å†å²æŸ¥è¯¢è¿”å›æ›´å¤šç»“æœ
                )
                
                documents = self._extract_documents_from_tool_results(results)
                
                # æŒ‰æ—¶é—´æ’åºï¼ˆå¦‚æœæœ‰æ—¶é—´æˆ³ï¼‰
                sorted_docs = sorted(
                    documents,
                    key=lambda x: x.get('timestamp', x.get('date', '')),
                    reverse=True
                )
                
                self._log_thought(
                    state,
                    "observation",
                    f"æ£€ç´¢åˆ° {len(sorted_docs)} æ¡å†å²è®°å½•"
                )
                
                report = self._format_historical_results(query, sorted_docs)
                
                return {
                    "answer": report,
                    "content": report,
                    "documents": sorted_docs,
                    "source_documents": sorted_docs,
                    "count": len(sorted_docs)
                }
            except Exception as e:
                logger.warning(f"Historical search failed: {e}")
                self._log_thought(state, "correction", f"å†å²æŸ¥è¯¢å¤±è´¥: {str(e)}")
                return self._fallback_response(query)
        else:
            return self._fallback_response(query)
    
    def _format_search_results(self, query: str, documents: List[Dict]) -> str:
        """æ ¼å¼åŒ–æ£€ç´¢ç»“æœ - ä¼˜åŒ–ç‰ˆ"""
        # æ ‡é¢˜å’Œæ¦‚è§ˆ
        report = f"# ğŸ“š çŸ¥è¯†æ£€ç´¢ç»“æœ\n\n"
        report += f"**ğŸ” æŸ¥è¯¢å†…å®¹**: {query}\n"
        report += f"**ğŸ“Š æ£€ç´¢ç»“æœ**: å…±æ‰¾åˆ° **{len(documents)}** æ¡ç›¸å…³æ–‡æ¡£\n"
        report += f"**â° æ£€ç´¢æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        
        report += "---\n\n"
        
        # æ˜¾ç¤ºå‰10æ¡ç»“æœ
        display_count = min(10, len(documents))
        
        for i, doc in enumerate(documents[:display_count]):
            # æ–‡æ¡£æ ‡é¢˜
            title = doc.get('title', 'æœªå‘½åæ–‡æ¡£')
            report += f"## ğŸ“„ {i+1}. {title}\n\n"
            
            # å…ƒæ•°æ®ä¿¡æ¯
            metadata_items = []
            
            # æ¥æº
            source = doc.get('source', doc.get('collection_name', 'çŸ¥è¯†åº“'))
            metadata_items.append(f"**ğŸ“ æ¥æº**: {source}")
            
            # ç±»å‹
            doc_type = doc.get('type', doc.get('category', ''))
            if doc_type:
                metadata_items.append(f"**ğŸ·ï¸ ç±»å‹**: {doc_type}")
            
            # æ—¶é—´
            timestamp = doc.get('timestamp', doc.get('date', doc.get('created_at', '')))
            if timestamp:
                metadata_items.append(f"**ğŸ“… æ—¶é—´**: {timestamp}")
            
            # ç›¸å…³åº¦åˆ†æ•°
            score = doc.get('score', doc.get('relevance_score', 0))
            if score > 0:
                score_percent = int(score * 100) if score <= 1 else int(score)
                score_bar = "ğŸŸ¢" if score_percent >= 80 else "ğŸŸ¡" if score_percent >= 60 else "ğŸ”´"
                metadata_items.append(f"**{score_bar} ç›¸å…³åº¦**: {score_percent}%")
            
            # æ˜¾ç¤ºå…ƒæ•°æ®
            report += " | ".join(metadata_items) + "\n\n"
            
            # å†…å®¹æ‘˜è¦
            content = doc.get('content', doc.get('text', ''))
            if content:
                # æ™ºèƒ½æˆªæ–­
                if len(content) > 300:
                    # å°è¯•åœ¨å¥å·å¤„æˆªæ–­
                    truncated = content[:300]
                    last_period = truncated.rfind('ã€‚')
                    if last_period > 200:  # å¦‚æœå¥å·ä½ç½®åˆç†
                        content = truncated[:last_period + 1]
                    else:
                        content = truncated + "..."
                
                report += f"**ğŸ’¡ å†…å®¹æ‘˜è¦**:\n\n"
                report += f"> {content}\n\n"
            
            # å…³é”®è¯/æ ‡ç­¾
            keywords = doc.get('keywords', doc.get('tags', []))
            if keywords:
                if isinstance(keywords, list):
                    keywords_str = " ".join([f"`{kw}`" for kw in keywords[:5]])
                else:
                    keywords_str = f"`{keywords}`"
                report += f"**ğŸ”– å…³é”®è¯**: {keywords_str}\n\n"
            
            report += "---\n\n"
        
        # æ˜¾ç¤ºæ›´å¤šæç¤º
        if len(documents) > display_count:
            remaining = len(documents) - display_count
            report += f"ğŸ“Œ *è¿˜æœ‰ **{remaining}** æ¡ç›¸å…³ç»“æœæœªæ˜¾ç¤º*\n\n"
        
        # æœç´¢å»ºè®®
        if len(documents) == 0:
            report += "ğŸ’¡ **æœç´¢å»ºè®®**:\n"
            report += "- å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯\n"
            report += "- ä½¿ç”¨æ›´å…·ä½“çš„è®¾å¤‡åç§°æˆ–ç¼–å·\n"
            report += "- æ£€æŸ¥æ‹¼å†™æ˜¯å¦æ­£ç¡®\n\n"
        elif len(documents) < 3:
            report += "ğŸ’¡ **æç¤º**: ç»“æœè¾ƒå°‘ï¼Œå¯ä»¥å°è¯•ä½¿ç”¨æ›´å®½æ³›çš„å…³é”®è¯\n\n"
        
        return report
    
    def _format_historical_results(self, query: str, documents: List[Dict]) -> str:
        """æ ¼å¼åŒ–å†å²ç»“æœ - ä¼˜åŒ–ç‰ˆ"""
        # æ ‡é¢˜å’Œæ¦‚è§ˆ
        report = f"# ğŸ“œ å†å²è®°å½•æŸ¥è¯¢ç»“æœ\n\n"
        report += f"**ğŸ” æŸ¥è¯¢å†…å®¹**: {query}\n"
        report += f"**ğŸ“Š æŸ¥è¯¢ç»“æœ**: å…±æ‰¾åˆ° **{len(documents)}** æ¡å†å²è®°å½•\n"
        report += f"**â° æŸ¥è¯¢æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        
        report += "---\n\n"
        
        # æŒ‰ç±»å‹åˆ†ç»„ç»Ÿè®¡
        type_stats = {}
        for doc in documents:
            doc_type = doc.get('type', doc.get('category', 'å…¶ä»–'))
            type_stats[doc_type] = type_stats.get(doc_type, 0) + 1
        
        if type_stats:
            report += "**ğŸ“ˆ è®°å½•ç±»å‹åˆ†å¸ƒ**:\n\n"
            for doc_type, count in sorted(type_stats.items(), key=lambda x: x[1], reverse=True):
                bar_length = min(20, int(count / len(documents) * 20))
                bar = "â–ˆ" * bar_length + "â–‘" * (20 - bar_length)
                report += f"- {doc_type}: {count} æ¡ {bar}\n"
            report += "\n---\n\n"
        
        # æ˜¾ç¤ºå‰15æ¡ç»“æœï¼ˆæ—¶é—´çº¿è§†å›¾ï¼‰
        display_count = min(15, len(documents))
        
        report += "## ğŸ“… æ—¶é—´çº¿è§†å›¾\n\n"
        
        for i, doc in enumerate(documents[:display_count]):
            # æ—¶é—´æˆ³
            timestamp = doc.get('timestamp', doc.get('date', 'æœªçŸ¥æ—¶é—´'))
            
            # æ–‡æ¡£æ ‡é¢˜å’Œç±»å‹
            title = doc.get('title', 'æœªå‘½åè®°å½•')
            doc_type = doc.get('type', doc.get('category', 'æœªåˆ†ç±»'))
            
            # ç±»å‹å›¾æ ‡
            type_icon = {
                'ç¼ºé™·': 'âš ï¸',
                'æ£€ä¿®': 'ğŸ”§',
                'å·¡è§†': 'ğŸ‘ï¸',
                'æ“ä½œ': 'âš¡',
                'è¯•éªŒ': 'ğŸ§ª',
                'äº‹æ•…': 'ğŸš¨',
                'æŠ¥å‘Š': 'ğŸ“‹',
                'è®°å½•': 'ğŸ“',
            }.get(doc_type, 'ğŸ“„')
            
            # æ—¶é—´çº¿èŠ‚ç‚¹
            report += f"### {type_icon} {timestamp}\n\n"
            report += f"**{i+1}. {title}**\n\n"
            
            # å…ƒæ•°æ®
            metadata_items = []
            metadata_items.append(f"**ğŸ·ï¸ ç±»å‹**: {doc_type}")
            
            # è®¾å¤‡ä¿¡æ¯
            equipment = doc.get('equipment', doc.get('device', ''))
            if equipment:
                metadata_items.append(f"**ğŸ”Œ è®¾å¤‡**: {equipment}")
            
            # è´£ä»»äºº
            responsible = doc.get('responsible', doc.get('operator', ''))
            if responsible:
                metadata_items.append(f"**ğŸ‘¤ è´£ä»»äºº**: {responsible}")
            
            # çŠ¶æ€
            status = doc.get('status', '')
            if status:
                status_icon = "âœ…" if status in ['å·²å®Œæˆ', 'æ­£å¸¸'] else "â³" if status in ['è¿›è¡Œä¸­', 'å¾…å¤„ç†'] else "âŒ"
                metadata_items.append(f"**{status_icon} çŠ¶æ€**: {status}")
            
            report += " | ".join(metadata_items) + "\n\n"
            
            # å†…å®¹æ‘˜è¦
            content = doc.get('content', doc.get('description', ''))
            if content:
                # æ™ºèƒ½æˆªæ–­
                if len(content) > 200:
                    truncated = content[:200]
                    last_period = truncated.rfind('ã€‚')
                    if last_period > 150:
                        content = truncated[:last_period + 1]
                    else:
                        content = truncated + "..."
                
                report += f"> {content}\n\n"
            
            # å…³é”®ä¿¡æ¯é«˜äº®
            severity = doc.get('severity', doc.get('level', ''))
            if severity:
                severity_color = {
                    'ç´§æ€¥': 'ğŸ”´',
                    'é‡è¦': 'ğŸŸ ',
                    'ä¸€èˆ¬': 'ğŸŸ¡',
                    'è½»å¾®': 'ğŸŸ¢',
                }.get(severity, 'âšª')
                report += f"{severity_color} **ä¸¥é‡ç¨‹åº¦**: {severity}\n\n"
            
            report += "---\n\n"
        
        # æ˜¾ç¤ºæ›´å¤šæç¤º
        if len(documents) > display_count:
            remaining = len(documents) - display_count
            report += f"ğŸ“Œ *è¿˜æœ‰ **{remaining}** æ¡å†å²è®°å½•æœªæ˜¾ç¤º*\n\n"
        
        # ç»Ÿè®¡æ‘˜è¦
        if len(documents) > 0:
            report += "## ğŸ“Š ç»Ÿè®¡æ‘˜è¦\n\n"
            
            # æ—¶é—´èŒƒå›´
            timestamps = [doc.get('timestamp', doc.get('date', '')) for doc in documents if doc.get('timestamp') or doc.get('date')]
            if timestamps:
                timestamps_sorted = sorted([t for t in timestamps if t])
                if timestamps_sorted:
                    report += f"- **æ—¶é—´èŒƒå›´**: {timestamps_sorted[0]} è‡³ {timestamps_sorted[-1]}\n"
            
            # è®°å½•æ€»æ•°
            report += f"- **è®°å½•æ€»æ•°**: {len(documents)} æ¡\n"
            
            # æœ€å¸¸è§ç±»å‹
            if type_stats:
                most_common_type = max(type_stats.items(), key=lambda x: x[1])
                report += f"- **æœ€å¸¸è§ç±»å‹**: {most_common_type[0]} ({most_common_type[1]} æ¡)\n"
            
            report += "\n"
        
        return report
    
    def _fallback_response(self, query: str) -> Dict[str, Any]:
        """å›é€€å“åº”ï¼ˆä½¿ç”¨Mockæ•°æ®ï¼‰"""
        # Mockæ•°æ®åº“
        mock_db = [
            {
                "id": "doc_001",
                "title": "1å·ä¸»å˜æ£€ä¿®è®°å½•_202405",
                "content": "2024å¹´5æœˆ12æ—¥ï¼Œå¯¹1å·ä¸»å˜è¿›è¡Œäº†ä¾‹è¡Œæ£€ä¿®ã€‚å‘ç°é«˜å‹ä¾§å¥—ç®¡æ²¹ä½ç•¥ä½ï¼Œå·²è¡¥æ²¹å¤„ç†ã€‚æœ¬ä½“æ²¹è‰²è°±åˆ†ææ­£å¸¸ã€‚",
                "source": "æ£€ä¿®è®°å½•åº“",
                "timestamp": "2024-05-12"
            },
            {
                "id": "kb_node_102",
                "title": "è®¾å¤‡å°è´¦: #1 ä¸»å˜å‹å™¨",
                "content": "å‹å·: SFZ11-110000/110; å‚å®¶: ç‰¹å˜ç”µå·¥; æŠ•è¿æ—¥æœŸ: 2015-06-01; å½“å‰çŠ¶æ€: è¿è¡Œä¸­ã€‚",
                "source": "è®¾å¤‡å°è´¦",
                "type": "è®¾å¤‡ä¿¡æ¯"
            },
            {
                "id": "rule_205",
                "title": "å˜ç”µå®‰è§„-å˜å‹å™¨ä½œä¸š",
                "content": "åœ¨å˜å‹å™¨ä¸Šä½œä¸šæ—¶ï¼Œå¿…é¡»æ–­å¼€ç”µæºï¼Œå¹¶æŒ‚å¥½æ¥åœ°çº¿ã€‚æ”€ç™»å˜å‹å™¨æ—¶åº”ä½©æˆ´å®‰å…¨å¸¦ã€‚",
                "source": "å®‰å…¨è§„ç¨‹",
                "type": "è§„ç¨‹æ–‡æ¡£"
            },
        ]
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        results = []
        query_lower = query.lower()
        for item in mock_db:
            if any(keyword in query_lower for keyword in ["ä¸»å˜", "å˜å‹å™¨", "æ£€ä¿®", "å°è´¦", "è§„ç¨‹"]):
                results.append(item)
        
        if not results:
            results = mock_db  # è¿”å›æ‰€æœ‰Mockæ•°æ®
        
        report = self._format_search_results(query, results)
        
        return {
            "answer": report,
            "content": report,
            "documents": results,
            "source_documents": results,
            "count": len(results),
            "note": "ä½¿ç”¨Mockæ•°æ®ï¼Œå®é™…éƒ¨ç½²æ—¶å°†è¿æ¥çœŸå®çŸ¥è¯†åº“"
        }
    
    def _extract_documents_from_tool_results(self, tool_results: List[Dict]) -> List[Dict]:
        """ä»å·¥å…·è°ƒç”¨ç»“æœä¸­æå–æ–‡æ¡£"""
        documents = []
        for result in tool_results:
            if isinstance(result, dict) and "result" in result:
                result_data = result["result"]
                if isinstance(result_data, dict) and "documents" in result_data:
                    documents.extend(result_data["documents"])
        return documents

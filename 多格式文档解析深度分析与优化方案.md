# å¤šæ ¼å¼æ–‡æ¡£è§£æç³»ç»Ÿæ·±åº¦åˆ†æä¸ä¼˜åŒ–æ–¹æ¡ˆ

## ğŸ“‹ ç›®å½•

1. [ç³»ç»Ÿæ¶æ„æ¦‚è¿°](#1-ç³»ç»Ÿæ¶æ„æ¦‚è¿°)
2. [ç°æœ‰è§£æå™¨åˆ†æ](#2-ç°æœ‰è§£æå™¨åˆ†æ)
3. [é—®é¢˜è¯†åˆ«ä¸æŒ‘æˆ˜](#3-é—®é¢˜è¯†åˆ«ä¸æŒ‘æˆ˜)
4. [ä¼˜åŒ–æ–¹æ¡ˆ](#4-ä¼˜åŒ–æ–¹æ¡ˆ)
5. [å®æ–½è·¯çº¿å›¾](#5-å®æ–½è·¯çº¿å›¾)

---

## 1. ç³»ç»Ÿæ¶æ„æ¦‚è¿°

### 1.1 æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Document Parsing Pipeline                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Input   â”‚ -> â”‚  DocParser  â”‚ -> â”‚   Parsers    â”‚ -> â”‚  Chunking   â”‚   â”‚
â”‚   â”‚  File    â”‚    â”‚ (Coordinator)â”‚    â”‚ (Specialized)â”‚    â”‚ (Rechunker) â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                              â”‚                               â”‚
â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚                          â†“                   â†“                   â†“          â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚                   â”‚ DocRay   â”‚        â”‚MarkItDownâ”‚        â”‚  Image   â”‚      â”‚
â”‚                   â”‚ Parser   â”‚        â”‚ Parser   â”‚        â”‚  Parser  â”‚      â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                          â”‚                   â”‚                   â”‚          â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚                   â”‚  MinerU  â”‚        â”‚  Audio   â”‚        â”‚  (More)  â”‚      â”‚
â”‚                   â”‚ Parser   â”‚        â”‚  Parser  â”‚        â”‚          â”‚      â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                           Output: List[Part] + Markdown                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æ ¸å¿ƒç»„ä»¶

| ç»„ä»¶ | æ–‡ä»¶ | èŒè´£ |
|------|------|------|
| **DocParser** | `doc_parser.py` | è§£æå™¨åè°ƒå™¨ï¼Œæ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åˆé€‚çš„è§£æå™¨ |
| **BaseParser** | `base.py` | è§£æå™¨æŠ½è±¡åŸºç±»ï¼Œå®šä¹‰ç»Ÿä¸€æ¥å£ |
| **Part æ•°æ®æ¨¡å‹** | `base.py` | æ–‡æ¡£éƒ¨åˆ†çš„æ•°æ®ç»“æ„ï¼ˆTextPart, TitlePart, ImagePartç­‰ï¼‰ |
| **Rechunker** | `chunking.py` | æ™ºèƒ½åˆ†å—ï¼ŒæŒ‰è¯­ä¹‰å’Œtokené™åˆ¶åˆ‡åˆ†æ–‡æ¡£ |
| **PartConverter** | `parse_md.py` | Markdownåˆ°Partå¯¹è±¡çš„è½¬æ¢å™¨ |

### 1.3 æ”¯æŒçš„æ–‡æ¡£æ ¼å¼ï¼ˆ30+ç§ï¼‰

| ç±»åˆ« | æ ¼å¼ | è§£æå™¨ |
|------|------|--------|
| **Officeæ–‡æ¡£** | .docx, .doc, .xlsx, .xls, .pptx, .ppt | DocRay / MarkItDown |
| **PDF** | .pdf | DocRay / MinerU / MarkItDown |
| **æ–‡æœ¬æ ¼å¼** | .txt, .md, .html, .htm, .ipynb | MarkItDown |
| **ç”µå­ä¹¦** | .epub | MarkItDown |
| **å›¾åƒ** | .jpg, .jpeg, .png, .bmp, .tiff, .tif | ImageParser |
| **éŸ³é¢‘/è§†é¢‘** | .mp3, .mp4, .wav, .webm, .ogg, .flac | AudioParser |

---

## 2. ç°æœ‰è§£æå™¨åˆ†æ

### 2.1 DocRayParser (é«˜çº§æ–‡æ¡£è§£æ)

**ä¼˜åŠ¿:**
- âœ… ä¸“ä¸šå¤„ç†å¤æ‚å¸ƒå±€PDFå’ŒOfficeæ–‡æ¡£
- âœ… ä¿ç•™æ–‡æ¡£ç»“æ„ä¿¡æ¯ï¼ˆé¡µé¢ã€å—ã€è¾¹ç•Œæ¡†ï¼‰
- âœ… æå–åµŒå…¥å›¾åƒå¹¶è½¬æ¢ä¸ºAssetBinPart
- âœ… æ”¯æŒè½®è¯¢æœºåˆ¶å¤„ç†å¤§æ–‡ä»¶

**åŠ£åŠ¿:**
- âŒ ä¾èµ–å¤–éƒ¨DocRayå¾®æœåŠ¡ï¼Œå¢åŠ éƒ¨ç½²å¤æ‚åº¦
- âŒ 5ç§’è½®è¯¢é—´éš”ï¼Œå¤§æ–‡ä»¶å¤„ç†å»¶è¿Ÿé«˜
- âŒ æœªå®ç°è¶…æ—¶æ§åˆ¶å’Œé‡è¯•æœºåˆ¶
- âŒ ä¸´æ—¶ç›®å½•ç®¡ç†å¯ä¼˜åŒ–

**ä»£ç é—®é¢˜:**
```python
# docray_parser.py:67 - å›ºå®š5ç§’è½®è¯¢é—´éš”ï¼Œå¯ä¼˜åŒ–ä¸ºæŒ‡æ•°é€€é¿
while True:
    time.sleep(5)  # Poll every 5 second
```

### 2.2 MarkItDownParser (é€šç”¨è§£æå™¨)

**ä¼˜åŠ¿:**
- âœ… æ”¯æŒæ ¼å¼æœ€å¤šï¼Œè¦†ç›–æ–‡æœ¬ã€Officeã€PDFã€ç”µå­ä¹¦
- âœ… ä½¿ç”¨æˆç†Ÿçš„markitdownåº“
- âœ… æ”¯æŒLibreOfficeæ ¼å¼è½¬æ¢

**åŠ£åŠ¿:**
- âŒ å¯¹å¤æ‚PDFå¸ƒå±€æ”¯æŒæœ‰é™
- âŒ æ ¼å¼è½¬æ¢ä¾èµ–sofficeå‘½ä»¤ï¼Œå¯èƒ½å¤±è´¥
- âŒ æœªæä¾›è½¬æ¢è¿›åº¦åé¦ˆ

### 2.3 ImageParser (å›¾åƒè§£æ)

**ä¼˜åŠ¿:**
- âœ… æ”¯æŒåŒOCRå¼•æ“ï¼ˆSiliconFlow + PaddleOCRï¼‰
- âœ… è‡ªåŠ¨é™çº§æœºåˆ¶

**åŠ£åŠ¿:**
- âŒ ä¸¤ä¸ªOCRæ–¹æ³•ä¸­æœ‰é‡å¤çš„`image_to_base64`å‡½æ•°å®šä¹‰
- âŒ å›¾åƒé¢„å¤„ç†ä¸è¶³ï¼ˆæ— æ—‹è½¬æ ¡æ­£ã€å¯¹æ¯”åº¦å¢å¼ºï¼‰
- âŒ æœªæ”¯æŒæ‰¹é‡å¤„ç†
- âŒ OCR promptç¡¬ç¼–ç ï¼Œä¸å¯é…ç½®

**ä»£ç é—®é¢˜:**
```python
# image_parser.py - é‡å¤å®šä¹‰image_to_base64å‡½æ•°
def read_image_text_siliconflow(self, path: Path) -> str:
    def image_to_base64(image_path: str) -> str:  # ç¬¬ä¸€æ¬¡å®šä¹‰
        ...

def read_image_text_paddleocr(self, path: Path) -> str:
    def image_to_base64(image_path: str):  # é‡å¤å®šä¹‰
        ...
```

### 2.4 AudioParser (éŸ³é¢‘è§£æ)

**ä¼˜åŠ¿:**
- âœ… ä½¿ç”¨Whisperè¿›è¡Œè¯­éŸ³è¯†åˆ«
- âœ… æ”¯æŒå¤šç§éŸ³é¢‘æ ¼å¼

**åŠ£åŠ¿:**
- âŒ æœªå®ç°éŸ³é¢‘åˆ†æ®µå¤„ç†ï¼Œé•¿éŸ³é¢‘å¯èƒ½è¶…æ—¶
- âŒ æœªæå–éŸ³é¢‘å…ƒæ•°æ®ï¼ˆæ—¶é•¿ã€é‡‡æ ·ç‡ç­‰ï¼‰
- âŒ ä¸æ”¯æŒå¤šè¯­è¨€è¯†åˆ«é…ç½®
- âŒ æ–‡ä»¶å¥æŸ„æœªæ­£ç¡®å…³é—­

**ä»£ç é—®é¢˜:**
```python
# audio_parser.py:59 - æ–‡ä»¶å¥æŸ„æœªæ­£ç¡®å…³é—­
files = {"audio_file": open(str(path), "rb")}
# åº”è¯¥ä½¿ç”¨ with è¯­å¥æˆ–åœ¨ finally ä¸­å…³é—­
```

### 2.5 MinerUParser (é«˜çº§PDFè§£æ)

**ä¼˜åŠ¿:**
- âœ… æ”¯æŒå¤æ‚PDFå¸ƒå±€å’Œå›¾åƒè§£æ
- âœ… äº‘ç«¯APIï¼Œæ— éœ€æœ¬åœ°éƒ¨ç½²

**åŠ£åŠ¿:**
- âŒ é»˜è®¤ç¦ç”¨ï¼Œéœ€è¦é¢å¤–é…ç½®
- âŒ ä¾èµ–å¤–éƒ¨APIï¼Œç½‘ç»œä¸ç¨³å®šæ—¶å½±å“å¯ç”¨æ€§
- âŒ ä¸‹è½½å’Œå¤„ç†ZIPåŒ…çš„å¤æ‚é€»è¾‘

---

## 3. é—®é¢˜è¯†åˆ«ä¸æŒ‘æˆ˜

### 3.1 æ¶æ„å±‚é¢é—®é¢˜

| é—®é¢˜ | å½±å“ | ä¼˜å…ˆçº§ |
|------|------|--------|
| **ç¼ºä¹ç»Ÿä¸€çš„é”™è¯¯å¤„ç†ç­–ç•¥** | è§£æå¤±è´¥éš¾ä»¥è¯Šæ–­å’Œæ¢å¤ | ğŸ”´ é«˜ |
| **è§£æå™¨é€‰æ‹©é€»è¾‘ç®€å•** | æ— æ³•æ ¹æ®æ–‡æ¡£ç‰¹å¾æ™ºèƒ½é€‰æ‹©æœ€ä½³è§£æå™¨ | ğŸŸ¡ ä¸­ |
| **æ— è§£æè´¨é‡è¯„ä¼°æœºåˆ¶** | æ— æ³•åˆ¤æ–­è§£æç»“æœçš„è´¨é‡ | ğŸŸ¡ ä¸­ |
| **ç¼ºä¹å¹¶è¡Œå¤„ç†èƒ½åŠ›** | å¤§æ‰¹é‡æ–‡æ¡£å¤„ç†æ•ˆç‡ä½ | ğŸ”´ é«˜ |
| **å†…å­˜ç®¡ç†ä¸è¶³** | å¤§æ–‡ä»¶å¤„ç†å¯èƒ½å¯¼è‡´OOM | ğŸ”´ é«˜ |

### 3.2 åŠŸèƒ½å±‚é¢é—®é¢˜

| é—®é¢˜ | å½±å“ | ä¼˜å…ˆçº§ |
|------|------|--------|
| **è¡¨æ ¼è§£ææ•ˆæœå·®** | ç»“æ„åŒ–æ•°æ®ä¸¢å¤± | ğŸ”´ é«˜ |
| **å…¬å¼æå–ä¸å®Œæ•´** | æŠ€æœ¯æ–‡æ¡£ç†è§£å—é™ | ğŸŸ¡ ä¸­ |
| **ç‰ˆå¼ä¿¡æ¯ä¸¢å¤±** | æ— æ³•è¿˜åŸåŸå§‹å¸ƒå±€ | ğŸŸ¢ ä½ |
| **å›¾åƒæè¿°ä¸è¶³** | ä¾èµ–OCRï¼Œç¼ºä¹è¯­ä¹‰ç†è§£ | ğŸŸ¡ ä¸­ |
| **ä¸æ”¯æŒåŠ å¯†æ–‡æ¡£** | éƒ¨åˆ†ä¸šåŠ¡æ–‡æ¡£æ— æ³•å¤„ç† | ğŸŸ¡ ä¸­ |

### 3.3 æ€§èƒ½å±‚é¢é—®é¢˜

| é—®é¢˜ | å½±å“ | ä¼˜å…ˆçº§ |
|------|------|--------|
| **åŒæ­¥é˜»å¡IO** | å¹¶å‘èƒ½åŠ›å—é™ | ğŸ”´ é«˜ |
| **æ— ç¼“å­˜æœºåˆ¶** | é‡å¤è§£ææ¶ˆè€—èµ„æº | ğŸŸ¡ ä¸­ |
| **æ— è¿›åº¦åé¦ˆ** | ç”¨æˆ·ä½“éªŒå·® | ğŸŸ¡ ä¸­ |
| **èµ„æºæ¸…ç†ä¸åŠæ—¶** | ä¸´æ—¶æ–‡ä»¶å †ç§¯ | ğŸŸ¢ ä½ |

---

## 4. ä¼˜åŒ–æ–¹æ¡ˆ

### 4.1 æ¶æ„ä¼˜åŒ–ï¼šå¼•å…¥è§£æå™¨ç®¡é“æ¨¡å¼

```python
# å»ºè®®æ–°å¢: aperag/docparser/pipeline.py

from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import List, Optional, Any
import asyncio
from enum import Enum

class ParsingStage(Enum):
    PRE_PROCESS = "pre_process"
    PARSE = "parse"
    POST_PROCESS = "post_process"
    VALIDATE = "validate"

@dataclass
class ParsingContext:
    """è§£æä¸Šä¸‹æ–‡ï¼Œè´¯ç©¿æ•´ä¸ªè§£æç®¡é“"""
    file_path: str
    file_size: int
    file_type: str
    metadata: dict
    parts: List[Any] = None
    errors: List[str] = None
    warnings: List[str] = None
    quality_score: float = 0.0
    parse_time_ms: int = 0
    
    def __post_init__(self):
        self.parts = self.parts or []
        self.errors = self.errors or []
        self.warnings = self.warnings or []

class ParsingPipelineStage(ABC):
    """è§£æç®¡é“é˜¶æ®µåŸºç±»"""
    
    @property
    @abstractmethod
    def stage(self) -> ParsingStage:
        pass
    
    @abstractmethod
    async def process(self, context: ParsingContext) -> ParsingContext:
        pass
    
    def should_skip(self, context: ParsingContext) -> bool:
        return False

class EnhancedDocParser:
    """å¢å¼ºç‰ˆæ–‡æ¡£è§£æå™¨"""
    
    def __init__(self):
        self.stages: List[ParsingPipelineStage] = []
        self.fallback_parsers: List[BaseParser] = []
        
    def add_stage(self, stage: ParsingPipelineStage) -> "EnhancedDocParser":
        self.stages.append(stage)
        return self
    
    async def parse_async(self, file_path: str, metadata: dict = None) -> ParsingContext:
        """å¼‚æ­¥è§£ææ–‡æ¡£"""
        import time
        start_time = time.time()
        
        context = ParsingContext(
            file_path=file_path,
            file_size=os.path.getsize(file_path),
            file_type=Path(file_path).suffix.lower(),
            metadata=metadata or {}
        )
        
        for stage in self.stages:
            if stage.should_skip(context):
                continue
            try:
                context = await stage.process(context)
            except Exception as e:
                context.errors.append(f"Stage {stage.stage.value} failed: {str(e)}")
                if stage.stage == ParsingStage.PARSE:
                    # å°è¯•é™çº§å¤„ç†
                    context = await self._fallback_parse(context)
        
        context.parse_time_ms = int((time.time() - start_time) * 1000)
        return context
    
    async def _fallback_parse(self, context: ParsingContext) -> ParsingContext:
        """é™çº§è§£æ"""
        for parser in self.fallback_parsers:
            try:
                parts = parser.parse_file(Path(context.file_path), context.metadata)
                context.parts = parts
                context.warnings.append(f"Fell back to {parser.__class__.__name__}")
                return context
            except Exception:
                continue
        context.errors.append("All fallback parsers failed")
        return context
```

### 4.2 é¢„å¤„ç†ä¼˜åŒ–ï¼šæ™ºèƒ½æ–‡æ¡£åˆ†æ

```python
# å»ºè®®æ–°å¢: aperag/docparser/preprocessor.py

import magic
from PIL import Image
import fitz  # PyMuPDF
from dataclasses import dataclass
from typing import Tuple, List

@dataclass
class DocumentAnalysis:
    """æ–‡æ¡£åˆ†æç»“æœ"""
    actual_mime_type: str
    has_images: bool
    has_tables: bool
    has_text_layer: bool  # PDFæ˜¯å¦æœ‰æ–‡æœ¬å±‚
    page_count: int
    estimated_complexity: str  # low, medium, high
    recommended_parser: str
    requires_ocr: bool
    language_hints: List[str]
    file_integrity: bool

class DocumentPreprocessor:
    """æ–‡æ¡£é¢„å¤„ç†å™¨"""
    
    def analyze(self, file_path: str) -> DocumentAnalysis:
        """åˆ†ææ–‡æ¡£ç‰¹å¾ï¼Œä¸ºè§£æå™¨é€‰æ‹©æä¾›ä¾æ®"""
        mime = magic.from_file(file_path, mime=True)
        suffix = Path(file_path).suffix.lower()
        
        if suffix == ".pdf" or mime == "application/pdf":
            return self._analyze_pdf(file_path)
        elif suffix in [".jpg", ".jpeg", ".png", ".bmp", ".tiff", ".tif"]:
            return self._analyze_image(file_path)
        elif suffix in [".docx", ".doc"]:
            return self._analyze_word(file_path)
        else:
            return self._default_analysis(file_path, mime)
    
    def _analyze_pdf(self, file_path: str) -> DocumentAnalysis:
        """åˆ†æPDFæ–‡æ¡£"""
        doc = fitz.open(file_path)
        
        has_images = False
        has_tables = False
        has_text = False
        total_text_len = 0
        
        for page in doc:
            # æ£€æŸ¥æ–‡æœ¬å±‚
            text = page.get_text()
            total_text_len += len(text)
            if len(text.strip()) > 50:
                has_text = True
            
            # æ£€æŸ¥å›¾åƒ
            images = page.get_images()
            if images:
                has_images = True
            
            # ç®€å•è¡¨æ ¼æ£€æµ‹ï¼ˆåŸºäºè¡¨æ ¼çº¿ï¼‰
            # TODO: æ›´ç²¾ç¡®çš„è¡¨æ ¼æ£€æµ‹
        
        # åˆ¤æ–­å¤æ‚åº¦
        complexity = "low"
        if has_images and has_tables:
            complexity = "high"
        elif has_images or has_tables:
            complexity = "medium"
        
        # æ¨èè§£æå™¨
        if not has_text or total_text_len < 100:
            recommended = "mineru"  # éœ€è¦OCR
            requires_ocr = True
        elif complexity == "high":
            recommended = "docray"
        else:
            recommended = "markitdown"
            requires_ocr = False
        
        doc.close()
        
        return DocumentAnalysis(
            actual_mime_type="application/pdf",
            has_images=has_images,
            has_tables=has_tables,
            has_text_layer=has_text,
            page_count=doc.page_count,
            estimated_complexity=complexity,
            recommended_parser=recommended,
            requires_ocr=requires_ocr,
            language_hints=self._detect_language(text),
            file_integrity=True
        )
    
    def _detect_language(self, text: str) -> List[str]:
        """æ£€æµ‹æ–‡æœ¬è¯­è¨€"""
        # ç®€å•å¯å‘å¼æ£€æµ‹
        hints = []
        if any('\u4e00' <= c <= '\u9fff' for c in text):
            hints.append("zh")
        if any('a' <= c.lower() <= 'z' for c in text):
            hints.append("en")
        return hints or ["unknown"]
    
    def _analyze_image(self, file_path: str) -> DocumentAnalysis:
        """åˆ†æå›¾åƒæ–‡æ¡£"""
        img = Image.open(file_path)
        
        return DocumentAnalysis(
            actual_mime_type=f"image/{img.format.lower() if img.format else 'unknown'}",
            has_images=True,
            has_tables=False,
            has_text_layer=False,
            page_count=1,
            estimated_complexity="low",
            recommended_parser="image",
            requires_ocr=True,
            language_hints=[],
            file_integrity=True
        )
```

### 4.3 å›¾åƒè§£æä¼˜åŒ–

```python
# å»ºè®®ä¿®æ”¹: aperag/docparser/image_parser.py

import base64
import asyncio
from io import BytesIO
from pathlib import Path
from typing import Any, Optional
from concurrent.futures import ThreadPoolExecutor

import numpy as np
from PIL import Image, ImageEnhance, ImageFilter
import requests

from aperag.config import settings
from aperag.docparser.base import BaseParser, FallbackError, Part, TextPart

class EnhancedImageParser(BaseParser):
    """å¢å¼ºç‰ˆå›¾åƒè§£æå™¨"""
    
    name = "enhanced_image"
    
    SUPPORTED_EXTENSIONS = [".jpg", ".jpeg", ".png", ".bmp", ".tiff", ".tif", ".webp"]
    
    def __init__(
        self,
        enable_preprocessing: bool = True,
        enable_vision_llm: bool = True,
        max_image_dimension: int = 4096,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.enable_preprocessing = enable_preprocessing
        self.enable_vision_llm = enable_vision_llm
        self.max_image_dimension = max_image_dimension
        self._executor = ThreadPoolExecutor(max_workers=4)
    
    def supported_extensions(self) -> list[str]:
        return self.SUPPORTED_EXTENSIONS
    
    def preprocess_image(self, image: Image.Image) -> Image.Image:
        """å›¾åƒé¢„å¤„ç†ï¼Œæé«˜OCRå‡†ç¡®ç‡"""
        # 1. å°ºå¯¸è°ƒæ•´
        if max(image.size) > self.max_image_dimension:
            ratio = self.max_image_dimension / max(image.size)
            new_size = (int(image.width * ratio), int(image.height * ratio))
            image = image.resize(new_size, Image.Resampling.LANCZOS)
        
        # 2. è½¬æ¢ä¸ºRGBæ¨¡å¼
        if image.mode in ("RGBA", "P"):
            background = Image.new("RGB", image.size, (255, 255, 255))
            if image.mode == "RGBA":
                background.paste(image, mask=image.split()[3])
            else:
                background.paste(image)
            image = background
        elif image.mode != "RGB":
            image = image.convert("RGB")
        
        # 3. è‡ªåŠ¨æ—‹è½¬ï¼ˆåŸºäºEXIFï¼‰
        try:
            exif = image._getexif()
            if exif:
                orientation = exif.get(274)  # Orientation tag
                if orientation == 3:
                    image = image.rotate(180, expand=True)
                elif orientation == 6:
                    image = image.rotate(270, expand=True)
                elif orientation == 8:
                    image = image.rotate(90, expand=True)
        except (AttributeError, KeyError):
            pass
        
        # 4. å¢å¼ºå¯¹æ¯”åº¦ï¼ˆå¯é€‰ï¼‰
        if self.enable_preprocessing:
            enhancer = ImageEnhance.Contrast(image)
            image = enhancer.enhance(1.2)
            
            # è½»å¾®é”åŒ–
            image = image.filter(ImageFilter.UnsharpMask(radius=1, percent=50))
        
        return image
    
    @staticmethod
    def image_to_base64(image: Image.Image) -> str:
        """ç»Ÿä¸€çš„å›¾åƒè½¬base64æ–¹æ³•"""
        buffered = BytesIO()
        image.save(buffered, format="JPEG", quality=95)
        return base64.b64encode(buffered.getvalue()).decode()
    
    def parse_file(self, path: Path, metadata: dict[str, Any] = {}, **kwargs) -> list[Part]:
        """è§£æå›¾åƒæ–‡ä»¶"""
        image = Image.open(path)
        processed_image = self.preprocess_image(image)
        
        results = []
        
        # 1. OCRæå–æ–‡æœ¬
        ocr_text = self._extract_text_ocr(processed_image)
        if ocr_text:
            results.append(("ocr", ocr_text))
        
        # 2. Vision LLMæè¿°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.enable_vision_llm:
            vision_text = self._extract_vision_description(processed_image)
            if vision_text:
                results.append(("vision", vision_text))
        
        # åˆå¹¶ç»“æœ
        combined_content = self._merge_results(results)
        
        meta = metadata.copy()
        meta["md_source_map"] = [0, combined_content.count("\n") + 1]
        meta["image_size"] = image.size
        meta["extraction_methods"] = [r[0] for r in results]
        
        return [TextPart(content=combined_content, metadata=meta)]
    
    def _extract_text_ocr(self, image: Image.Image) -> Optional[str]:
        """OCRæ–‡æœ¬æå–"""
        b64_image = self.image_to_base64(image)
        
        # ä¼˜å…ˆä½¿ç”¨SiliconFlow
        if settings.siliconflow_ocr_api_key:
            try:
                return self._ocr_siliconflow(b64_image)
            except Exception as e:
                logger.warning(f"SiliconFlow OCR failed: {e}")
        
        # é™çº§åˆ°PaddleOCR
        if settings.paddleocr_host:
            try:
                return self._ocr_paddleocr(b64_image)
            except Exception as e:
                logger.warning(f"PaddleOCR failed: {e}")
        
        return None
    
    def _ocr_siliconflow(self, b64_image: str) -> str:
        """SiliconFlow OCR"""
        payload = {
            "model": settings.siliconflow_ocr_model,
            "messages": [{
                "role": "user",
                "content": [
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64_image}", "detail": "high"}},
                    {"type": "text", "text": self._get_ocr_prompt()}
                ]
            }]
        }
        
        response = requests.post(
            f"{settings.siliconflow_ocr_base_url.rstrip('/')}/chat/completions",
            headers={"Authorization": f"Bearer {settings.siliconflow_ocr_api_key}", "Content-Type": "application/json"},
            json=payload,
            timeout=60
        )
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"].strip()
    
    def _get_ocr_prompt(self) -> str:
        """å¯é…ç½®çš„OCRæç¤ºè¯"""
        # TODO: ä»é…ç½®æ–‡ä»¶è¯»å–
        return """è¯·æå–å›¾åƒä¸­çš„æ‰€æœ‰å¯è§æ–‡å­—ï¼Œä¿æŒåŸå§‹è¯­è¨€å’Œæ ¼å¼ã€‚
è¦æ±‚ï¼š
1. å®Œæ•´æå–æ‰€æœ‰æ–‡æœ¬ï¼Œä¸é—æ¼
2. ä¿æŒåŸå§‹æ’ç‰ˆæ ¼å¼
3. å¦‚æœæœ‰è¡¨æ ¼ï¼Œä½¿ç”¨Markdownæ ¼å¼è¾“å‡º
4. åªè¿”å›æå–çš„æ–‡å­—å†…å®¹ï¼Œä¸è¦æ·»åŠ è§£é‡Š"""
    
    def _extract_vision_description(self, image: Image.Image) -> Optional[str]:
        """ä½¿ç”¨Vision LLMç”Ÿæˆå›¾åƒæè¿°"""
        # TODO: é›†æˆVision LLMæœåŠ¡
        pass
    
    def _merge_results(self, results: list) -> str:
        """æ™ºèƒ½åˆå¹¶OCRå’ŒVisionç»“æœ"""
        if not results:
            return ""
        
        if len(results) == 1:
            return results[0][1]
        
        # å¦‚æœåŒæ—¶æœ‰OCRå’ŒVisionç»“æœï¼Œæ™ºèƒ½åˆå¹¶
        ocr_text = next((r[1] for r in results if r[0] == "ocr"), "")
        vision_text = next((r[1] for r in results if r[0] == "vision"), "")
        
        if ocr_text and vision_text:
            return f"## å›¾åƒå†…å®¹\n\n{vision_text}\n\n## æå–æ–‡æœ¬\n\n{ocr_text}"
        
        return ocr_text or vision_text
```

### 4.4 åˆ†å—ç­–ç•¥ä¼˜åŒ–

```python
# å»ºè®®æ–°å¢: aperag/docparser/enhanced_chunking.py

from dataclasses import dataclass
from typing import List, Callable, Optional
from enum import Enum

class ChunkingStrategy(Enum):
    """åˆ†å—ç­–ç•¥"""
    FIXED_SIZE = "fixed_size"          # å›ºå®šå¤§å°
    SEMANTIC = "semantic"              # è¯­ä¹‰åˆ†å—
    HIERARCHICAL = "hierarchical"      # å±‚æ¬¡ç»“æ„åˆ†å—
    ADAPTIVE = "adaptive"              # è‡ªé€‚åº”åˆ†å—

@dataclass
class ChunkConfig:
    """åˆ†å—é…ç½®"""
    strategy: ChunkingStrategy = ChunkingStrategy.SEMANTIC
    chunk_size: int = 400
    chunk_overlap: int = 50
    min_chunk_size: int = 100
    max_chunk_size: int = 800
    preserve_code_blocks: bool = True  # ä¿æŒä»£ç å—å®Œæ•´
    preserve_tables: bool = True       # ä¿æŒè¡¨æ ¼å®Œæ•´
    include_metadata: bool = True      # åœ¨å—ä¸­åŒ…å«å…ƒæ•°æ®å‰ç¼€

class AdaptiveChunker:
    """è‡ªé€‚åº”åˆ†å—å™¨"""
    
    def __init__(self, config: ChunkConfig, tokenizer: Callable[[str], List[int]]):
        self.config = config
        self.tokenizer = tokenizer
    
    def chunk(self, parts: list[Part]) -> list[Part]:
        """æ ¹æ®å†…å®¹ç±»å‹è‡ªé€‚åº”åˆ†å—"""
        if self.config.strategy == ChunkingStrategy.ADAPTIVE:
            return self._adaptive_chunk(parts)
        elif self.config.strategy == ChunkingStrategy.HIERARCHICAL:
            return self._hierarchical_chunk(parts)
        else:
            # ä½¿ç”¨ç°æœ‰çš„rechunké€»è¾‘
            from aperag.docparser.chunking import rechunk
            return rechunk(parts, self.config.chunk_size, self.config.chunk_overlap, self.tokenizer)
    
    def _adaptive_chunk(self, parts: list[Part]) -> list[Part]:
        """è‡ªé€‚åº”åˆ†å—ï¼šæ ¹æ®å†…å®¹ç±»å‹é€‰æ‹©ä¸åŒç­–ç•¥"""
        result = []
        current_chunk_parts = []
        current_tokens = 0
        
        for part in parts:
            part_type = self._classify_part(part)
            
            # ç‰¹æ®Šå¤„ç†ï¼šä»£ç å—å’Œè¡¨æ ¼
            if part_type == "code" and self.config.preserve_code_blocks:
                # åˆ·æ–°å½“å‰å—
                if current_chunk_parts:
                    result.extend(self._finalize_chunk(current_chunk_parts))
                    current_chunk_parts = []
                    current_tokens = 0
                # ä»£ç å—å•ç‹¬æˆå—
                result.append(part)
                continue
            
            if part_type == "table" and self.config.preserve_tables:
                if current_chunk_parts:
                    result.extend(self._finalize_chunk(current_chunk_parts))
                    current_chunk_parts = []
                    current_tokens = 0
                result.append(part)
                continue
            
            # å¸¸è§„å†…å®¹
            part_tokens = len(self.tokenizer(part.content or ""))
            
            if current_tokens + part_tokens > self.config.max_chunk_size:
                # è¶…å‡ºé™åˆ¶ï¼Œå…ˆåˆ·æ–°
                if current_chunk_parts:
                    result.extend(self._finalize_chunk(current_chunk_parts))
                current_chunk_parts = [part]
                current_tokens = part_tokens
            else:
                current_chunk_parts.append(part)
                current_tokens += part_tokens
        
        # å¤„ç†å‰©ä½™
        if current_chunk_parts:
            result.extend(self._finalize_chunk(current_chunk_parts))
        
        return result
    
    def _classify_part(self, part: Part) -> str:
        """åˆ†ç±»Partç±»å‹"""
        from aperag.docparser.base import CodePart, TitlePart, ImagePart
        
        if isinstance(part, CodePart):
            return "code"
        if isinstance(part, TitlePart):
            return "title"
        if isinstance(part, ImagePart):
            return "image"
        
        content = part.content or ""
        if "| " in content and content.count("|") > 4:
            return "table"
        
        return "text"
    
    def _finalize_chunk(self, parts: list[Part]) -> list[Part]:
        """å®Œæˆå—å¤„ç†"""
        # è¿™é‡Œå¯ä»¥æ·»åŠ é‡å é€»è¾‘
        return parts
    
    def _hierarchical_chunk(self, parts: list[Part]) -> list[Part]:
        """å±‚æ¬¡ç»“æ„åˆ†å—ï¼šä¿æŒæ–‡æ¡£ç»“æ„"""
        # TODO: å®ç°åŸºäºæ ‡é¢˜å±‚çº§çš„åˆ†å—
        pass
```

### 4.5 å¹¶è¡Œå¤„ç†ä¼˜åŒ–

```python
# å»ºè®®æ–°å¢: aperag/docparser/concurrent.py

import asyncio
from typing import List, Dict, Any
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import multiprocessing

class ConcurrentDocParser:
    """å¹¶å‘æ–‡æ¡£è§£æå™¨"""
    
    def __init__(
        self,
        max_workers: int = None,
        use_process_pool: bool = False
    ):
        self.max_workers = max_workers or min(multiprocessing.cpu_count(), 4)
        self.use_process_pool = use_process_pool
    
    async def parse_batch(
        self,
        file_paths: List[str],
        metadata_list: List[Dict[str, Any]] = None,
        progress_callback = None
    ) -> List[Dict]:
        """æ‰¹é‡å¹¶å‘è§£ææ–‡æ¡£"""
        if metadata_list is None:
            metadata_list = [{}] * len(file_paths)
        
        results = []
        semaphore = asyncio.Semaphore(self.max_workers)
        
        async def parse_one(file_path: str, metadata: dict, index: int):
            async with semaphore:
                try:
                    from aperag.docparser.doc_parser import DocParser
                    parser = DocParser()
                    parts = parser.parse_file(Path(file_path), metadata)
                    result = {"success": True, "parts": parts, "file": file_path}
                except Exception as e:
                    result = {"success": False, "error": str(e), "file": file_path}
                
                if progress_callback:
                    await progress_callback(index, len(file_paths), result)
                
                return result
        
        tasks = [
            parse_one(fp, meta, i) 
            for i, (fp, meta) in enumerate(zip(file_paths, metadata_list))
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        return results
    
    async def parse_large_document(
        self,
        file_path: str,
        metadata: dict = None
    ) -> Dict:
        """å¤§æ–‡æ¡£åˆ†é¡µå¹¶è¡Œè§£æ"""
        from aperag.docparser.preprocessor import DocumentPreprocessor
        
        preprocessor = DocumentPreprocessor()
        analysis = preprocessor.analyze(file_path)
        
        if analysis.page_count <= 10:
            # å°æ–‡æ¡£ç›´æ¥è§£æ
            return await self._parse_single(file_path, metadata)
        
        # å¤§æ–‡æ¡£åˆ†é¡µå¤„ç†
        # TODO: å®ç°PDFåˆ†é¡µè§£æ
        pass
```

### 4.6 è´¨é‡è¯„ä¼°æœºåˆ¶

```python
# å»ºè®®æ–°å¢: aperag/docparser/quality.py

from dataclasses import dataclass
from typing import List, Optional
import re

@dataclass
class QualityMetrics:
    """è§£æè´¨é‡æŒ‡æ ‡"""
    text_coverage: float      # æ–‡æœ¬è¦†ç›–ç‡ (0-1)
    structure_score: float    # ç»“æ„ä¿ç•™åº¦ (0-1)
    encoding_quality: float   # ç¼–ç è´¨é‡ (0-1)
    completeness: float       # å®Œæ•´æ€§ (0-1)
    overall_score: float      # ç»¼åˆå¾—åˆ† (0-1)
    issues: List[str]         # å‘ç°çš„é—®é¢˜

class QualityEvaluator:
    """è§£æè´¨é‡è¯„ä¼°å™¨"""
    
    def evaluate(self, parts: list, original_file_path: str = None) -> QualityMetrics:
        """è¯„ä¼°è§£æè´¨é‡"""
        issues = []
        
        # 1. æ–‡æœ¬è¦†ç›–ç‡
        text_coverage = self._evaluate_text_coverage(parts)
        if text_coverage < 0.3:
            issues.append("æ–‡æœ¬æå–ç‡è¿‡ä½ï¼Œå¯èƒ½éœ€è¦OCR")
        
        # 2. ç»“æ„ä¿ç•™åº¦
        structure_score = self._evaluate_structure(parts)
        if structure_score < 0.5:
            issues.append("æ–‡æ¡£ç»“æ„æŸå¤±ä¸¥é‡")
        
        # 3. ç¼–ç è´¨é‡
        encoding_quality = self._evaluate_encoding(parts)
        if encoding_quality < 0.8:
            issues.append("å­˜åœ¨ç¼–ç é—®é¢˜æˆ–ä¹±ç ")
        
        # 4. å®Œæ•´æ€§
        completeness = self._evaluate_completeness(parts, original_file_path)
        if completeness < 0.7:
            issues.append("è§£æå¯èƒ½ä¸å®Œæ•´")
        
        # ç»¼åˆå¾—åˆ†
        overall = (text_coverage * 0.3 + structure_score * 0.2 + 
                   encoding_quality * 0.2 + completeness * 0.3)
        
        return QualityMetrics(
            text_coverage=text_coverage,
            structure_score=structure_score,
            encoding_quality=encoding_quality,
            completeness=completeness,
            overall_score=overall,
            issues=issues
        )
    
    def _evaluate_text_coverage(self, parts: list) -> float:
        """è¯„ä¼°æ–‡æœ¬è¦†ç›–ç‡"""
        total_content_len = sum(len(p.content or "") for p in parts)
        
        # æ ¹æ®å†…å®¹é‡è¯„ä¼°
        if total_content_len < 50:
            return 0.1
        elif total_content_len < 500:
            return 0.5
        elif total_content_len < 2000:
            return 0.8
        else:
            return 1.0
    
    def _evaluate_structure(self, parts: list) -> float:
        """è¯„ä¼°ç»“æ„ä¿ç•™åº¦"""
        from aperag.docparser.base import TitlePart, CodePart
        
        has_titles = any(isinstance(p, TitlePart) for p in parts)
        has_structure = len(parts) > 1
        
        score = 0.5
        if has_titles:
            score += 0.3
        if has_structure:
            score += 0.2
        
        return min(score, 1.0)
    
    def _evaluate_encoding(self, parts: list) -> float:
        """è¯„ä¼°ç¼–ç è´¨é‡"""
        total_chars = 0
        invalid_chars = 0
        
        for part in parts:
            content = part.content or ""
            total_chars += len(content)
            
            # æ£€æµ‹å¸¸è§ä¹±ç æ¨¡å¼
            invalid_patterns = [
                r'[ï¿½\ufffd]',  # æ›¿æ¢å­—ç¬¦
                r'[\x00-\x08\x0b\x0c\x0e-\x1f]',  # æ§åˆ¶å­—ç¬¦
            ]
            for pattern in invalid_patterns:
                invalid_chars += len(re.findall(pattern, content))
        
        if total_chars == 0:
            return 1.0
        
        return max(0, 1 - (invalid_chars / total_chars) * 10)
    
    def _evaluate_completeness(self, parts: list, file_path: str = None) -> float:
        """è¯„ä¼°å®Œæ•´æ€§"""
        if not parts:
            return 0.0
        
        # å¦‚æœæœ‰åŸæ–‡ä»¶ï¼Œå¯ä»¥æ¯”è¾ƒå¤§å°ç­‰
        # è¿™é‡Œä½¿ç”¨ç®€å•å¯å‘å¼
        total_content = sum(len(p.content or "") for p in parts)
        
        if total_content > 100:
            return 0.9
        elif total_content > 50:
            return 0.7
        else:
            return 0.5
```

### 4.7 ç¼“å­˜æœºåˆ¶

```python
# å»ºè®®æ–°å¢: aperag/docparser/cache.py

import hashlib
import json
import pickle
from pathlib import Path
from typing import Optional, List, Any
from datetime import datetime, timedelta
import aiofiles
import asyncio

class ParsingCache:
    """è§£æç»“æœç¼“å­˜"""
    
    def __init__(
        self,
        cache_dir: str = ".cache/parsed",
        ttl_hours: int = 24,
        max_size_mb: int = 1000
    ):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.ttl = timedelta(hours=ttl_hours)
        self.max_size = max_size_mb * 1024 * 1024
    
    def _get_cache_key(self, file_path: str, parser_config: dict = None) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        with open(file_path, "rb") as f:
            file_hash = hashlib.md5(f.read()).hexdigest()
        
        config_hash = ""
        if parser_config:
            config_hash = hashlib.md5(
                json.dumps(parser_config, sort_keys=True).encode()
            ).hexdigest()[:8]
        
        return f"{file_hash}_{config_hash}"
    
    def _get_cache_path(self, cache_key: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return self.cache_dir / f"{cache_key}.cache"
    
    async def get(self, file_path: str, parser_config: dict = None) -> Optional[List[Any]]:
        """è·å–ç¼“å­˜çš„è§£æç»“æœ"""
        cache_key = self._get_cache_key(file_path, parser_config)
        cache_path = self._get_cache_path(cache_key)
        
        if not cache_path.exists():
            return None
        
        # æ£€æŸ¥TTL
        mtime = datetime.fromtimestamp(cache_path.stat().st_mtime)
        if datetime.now() - mtime > self.ttl:
            cache_path.unlink()
            return None
        
        try:
            async with aiofiles.open(cache_path, "rb") as f:
                data = await f.read()
                return pickle.loads(data)
        except Exception:
            return None
    
    async def set(self, file_path: str, parts: List[Any], parser_config: dict = None):
        """ç¼“å­˜è§£æç»“æœ"""
        cache_key = self._get_cache_key(file_path, parser_config)
        cache_path = self._get_cache_path(cache_key)
        
        try:
            data = pickle.dumps(parts)
            async with aiofiles.open(cache_path, "wb") as f:
                await f.write(data)
        except Exception as e:
            logger.warning(f"Failed to cache parsing result: {e}")
    
    async def cleanup(self):
        """æ¸…ç†è¿‡æœŸå’Œè¶…é‡ç¼“å­˜"""
        files = sorted(
            self.cache_dir.glob("*.cache"),
            key=lambda p: p.stat().st_mtime
        )
        
        total_size = sum(f.stat().st_size for f in files)
        now = datetime.now()
        
        for f in files:
            # åˆ é™¤è¿‡æœŸæ–‡ä»¶
            mtime = datetime.fromtimestamp(f.stat().st_mtime)
            if now - mtime > self.ttl:
                f.unlink()
                total_size -= f.stat().st_size
                continue
            
            # æ§åˆ¶æ€»å¤§å°
            if total_size > self.max_size:
                f.unlink()
                total_size -= f.stat().st_size
```

---

## 5. å®æ–½è·¯çº¿å›¾

### 5.1 ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€ä¼˜åŒ– (1-2å‘¨)

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | é¢„ä¼°å·¥æ—¶ | è´Ÿè´£äºº |
|------|--------|----------|--------|
| ä¿®å¤ImageParseré‡å¤ä»£ç  | ğŸ”´ é«˜ | 2h | - |
| ä¿®å¤AudioParseræ–‡ä»¶å¥æŸ„æ³„æ¼ | ğŸ”´ é«˜ | 1h | - |
| æ·»åŠ ç»Ÿä¸€é”™è¯¯å¤„ç† | ğŸ”´ é«˜ | 4h | - |
| å®ç°è§£æè¶…æ—¶æœºåˆ¶ | ğŸ”´ é«˜ | 4h | - |
| æ·»åŠ åŸºç¡€æ—¥å¿—å¢å¼º | ğŸŸ¡ ä¸­ | 2h | - |

### 5.2 ç¬¬äºŒé˜¶æ®µï¼šåŠŸèƒ½å¢å¼º (2-3å‘¨)

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | é¢„ä¼°å·¥æ—¶ | è´Ÿè´£äºº |
|------|--------|----------|--------|
| å®ç°DocumentPreprocessor | ğŸ”´ é«˜ | 8h | - |
| å®ç°EnhancedImageParser | ğŸ”´ é«˜ | 8h | - |
| æ·»åŠ è´¨é‡è¯„ä¼°æœºåˆ¶ | ğŸŸ¡ ä¸­ | 8h | - |
| ä¼˜åŒ–åˆ†å—ç­–ç•¥ | ğŸŸ¡ ä¸­ | 8h | - |
| æ·»åŠ è§£æç¼“å­˜ | ğŸŸ¡ ä¸­ | 4h | - |

### 5.3 ç¬¬ä¸‰é˜¶æ®µï¼šæ€§èƒ½ä¼˜åŒ– (2å‘¨)

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | é¢„ä¼°å·¥æ—¶ | è´Ÿè´£äºº |
|------|--------|----------|--------|
| å®ç°å¹¶å‘æ‰¹å¤„ç† | ğŸ”´ é«˜ | 8h | - |
| ä¼˜åŒ–å†…å­˜ä½¿ç”¨ | ğŸ”´ é«˜ | 8h | - |
| æ·»åŠ è¿›åº¦åé¦ˆ | ğŸŸ¡ ä¸­ | 4h | - |
| å®ç°è§£æå™¨ç®¡é“æ¨¡å¼ | ğŸŸ¡ ä¸­ | 12h | - |

### 5.4 ç¬¬å››é˜¶æ®µï¼šæ‰©å±•æ”¯æŒ (2å‘¨)

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | é¢„ä¼°å·¥æ—¶ | è´Ÿè´£äºº |
|------|--------|----------|--------|
| æ·»åŠ CADæ ¼å¼æ”¯æŒ | ğŸŸ¢ ä½ | 16h | - |
| æ·»åŠ è§†é¢‘å¸§æå– | ğŸŸ¢ ä½ | 8h | - |
| æ·»åŠ åŠ å¯†æ–‡æ¡£æ”¯æŒ | ğŸŸ¡ ä¸­ | 8h | - |
| æ·»åŠ æ›´å¤šç”µå­ä¹¦æ ¼å¼ | ğŸŸ¢ ä½ | 8h | - |

---

## 6. ç›‘æ§ä¸æŒ‡æ ‡

### 6.1 å…³é”®æŒ‡æ ‡

```python
# å»ºè®®æ·»åŠ åˆ°ç›‘æ§ç³»ç»Ÿ

PARSING_METRICS = {
    # æ€§èƒ½æŒ‡æ ‡
    "parse_duration_seconds": Histogram,     # è§£æè€—æ—¶
    "parse_memory_bytes": Gauge,             # å†…å­˜ä½¿ç”¨
    "parse_queue_length": Gauge,             # é˜Ÿåˆ—é•¿åº¦
    
    # è´¨é‡æŒ‡æ ‡
    "parse_quality_score": Histogram,        # è´¨é‡å¾—åˆ†åˆ†å¸ƒ
    "parse_fallback_count": Counter,         # é™çº§æ¬¡æ•°
    
    # é”™è¯¯æŒ‡æ ‡
    "parse_error_count": Counter,            # é”™è¯¯è®¡æ•°
    "parse_timeout_count": Counter,          # è¶…æ—¶è®¡æ•°
    
    # ä¸šåŠ¡æŒ‡æ ‡
    "documents_parsed_total": Counter,       # æ€»è§£ææ–‡æ¡£æ•°
    "documents_by_type": Counter,            # æŒ‰ç±»å‹ç»Ÿè®¡
}
```

### 6.2 å‘Šè­¦è§„åˆ™

```yaml
alerts:
  - name: HighParseErrorRate
    condition: parse_error_count / documents_parsed_total > 0.1
    severity: warning
    
  - name: ParseQueueBacklog
    condition: parse_queue_length > 100
    severity: critical
    
  - name: LowQualityParsing
    condition: avg(parse_quality_score) < 0.5
    severity: warning
```

---

## 7. æ€»ç»“

æœ¬ä¼˜åŒ–æ–¹æ¡ˆä»æ¶æ„ã€åŠŸèƒ½ã€æ€§èƒ½ä¸‰ä¸ªç»´åº¦å¯¹ApeRAGv2çš„æ–‡æ¡£è§£æç³»ç»Ÿè¿›è¡Œäº†å…¨é¢åˆ†æå’Œä¼˜åŒ–è®¾è®¡ã€‚æ ¸å¿ƒä¼˜åŒ–ç‚¹åŒ…æ‹¬ï¼š

1. **æ¶æ„ä¼˜åŒ–**ï¼šå¼•å…¥ç®¡é“æ¨¡å¼ï¼Œæ”¯æŒçµæ´»çš„è§£ææµç¨‹ç¼–æ’
2. **æ™ºèƒ½é¢„å¤„ç†**ï¼šè‡ªåŠ¨åˆ†ææ–‡æ¡£ç‰¹å¾ï¼Œé€‰æ‹©æœ€ä½³è§£æç­–ç•¥
3. **å¢å¼ºè§£æèƒ½åŠ›**ï¼šä¼˜åŒ–å›¾åƒOCRã€æ”¹è¿›åˆ†å—ç­–ç•¥ã€æå‡è¡¨æ ¼è§£æ
4. **å¹¶å‘å¤„ç†**ï¼šæ”¯æŒæ‰¹é‡å¹¶å‘è§£æï¼Œæå‡ååé‡
5. **è´¨é‡ä¿éšœ**ï¼šå¼•å…¥è§£æè´¨é‡è¯„ä¼°å’Œç¼“å­˜æœºåˆ¶

å»ºè®®æŒ‰ç…§å®æ–½è·¯çº¿å›¾åˆ†é˜¶æ®µå®æ–½ï¼Œä¼˜å…ˆè§£å†³é«˜ä¼˜å…ˆçº§é—®é¢˜ï¼Œé€æ­¥å®Œå–„ç³»ç»Ÿèƒ½åŠ›ã€‚

---

*æ–‡æ¡£ç‰ˆæœ¬: 1.0*
*æ›´æ–°æ—¥æœŸ: 2025-12-06*
*ä½œè€…: AI Assistant*

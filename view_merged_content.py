#!/usr/bin/env python3
"""
æŸ¥çœ‹OCRæ–‡æœ¬å’ŒVision-to-Textåˆå¹¶åçš„å†…å®¹
"""

import sys
import os
import json
from datetime import datetime

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

try:
    from sqlalchemy import select, and_
    from aperag.db.models import Document, DocumentIndex, DocumentIndexType, Collection
    from aperag.config import get_sync_session
    from aperag.utils.utils import generate_vector_db_collection_name
    from aperag.config import get_vector_db_connector
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    sys.exit(1)


def view_merged_content(document_id: str):
    """æŸ¥çœ‹OCRå’ŒVision-to-Textåˆå¹¶åçš„å†…å®¹"""

    print("=" * 80)
    print("OCRæ–‡æœ¬ä¸Vision-to-Textå†…å®¹åˆå¹¶æŸ¥çœ‹å·¥å…·")
    print("=" * 80)
    print(f"\næ–‡æ¡£ID: {document_id}\n")

    for session in get_sync_session():
        # 1. è·å–æ–‡æ¡£ä¿¡æ¯
        doc = session.execute(
            select(Document).where(Document.id == document_id)
        ).scalar_one_or_none()

        if not doc:
            print("âŒ æ–‡æ¡£ä¸å­˜åœ¨")
            return

        collection = session.execute(
            select(Collection).where(Collection.id == doc.collection_id)
        ).scalar_one_or_none()

        if not collection:
            print("âŒ Collectionä¸å­˜åœ¨")
            return

        print(f"ğŸ“„ æ–‡æ¡£ä¿¡æ¯:")
        print(f"   åç§°: {doc.name}")
        print(f"   Collection: {collection.title}")
        print(f"   åˆ›å»ºæ—¶é—´: {doc.gmt_created}")

        # 2. è·å–å‘é‡ç´¢å¼•ï¼ˆåŒ…å«OCRæ–‡æœ¬ï¼‰
        vector_index = session.execute(
            select(DocumentIndex).where(
                and_(
                    DocumentIndex.document_id == document_id,
                    DocumentIndex.index_type == DocumentIndexType.VECTOR
                )
            )
        ).scalar_one_or_none()

        # 3. è·å–Visionç´¢å¼•
        vision_index = session.execute(
            select(DocumentIndex).where(
                and_(
                    DocumentIndex.document_id == document_id,
                    DocumentIndex.index_type == DocumentIndexType.VISION
                )
            )
        ).scalar_one_or_none()

        collection_name = generate_vector_db_collection_name(
            collection_id=collection.id
        )
        vector_store = get_vector_db_connector(collection=collection_name)
        qdrant_client = vector_store.connector.client

        # 4. æ˜¾ç¤ºOCRæ–‡æœ¬ï¼ˆä»å‘é‡ç´¢å¼•ï¼‰
        print(f"\n{'='*80}")
        print("ğŸ“ OCRæ–‡æœ¬å†…å®¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰")
        print("="*80)

        ocr_found = False
        if vector_index and vector_index.index_data:
            try:
                index_data = json.loads(vector_index.index_data)
                ctx_ids = index_data.get("context_ids", [])

                if ctx_ids:
                    points = qdrant_client.retrieve(
                        collection_name=collection_name,
                        ids=ctx_ids[:5],  # åªå–å‰5ä¸ª
                        with_payload=True,
                    )

                    for point in points:
                        if point.payload:
                            # æ£€æŸ¥æ˜¯å¦æ˜¯OCRæ–‡æœ¬
                            node_content = point.payload.get("_node_content")
                            if node_content:
                                try:
                                    payload_data = json.loads(node_content)
                                    metadata = payload_data.get("metadata", {})

                                    # OCRæ–‡æœ¬çš„æ ‡è¯†ï¼šsource == "ocr"
                                    if metadata.get("source") == "ocr":
                                        ocr_found = True
                                        text = payload_data.get("text", "")
                                        print(f"\nâœ… æ‰¾åˆ°OCRæ–‡æœ¬:")
                                        print(
                                            f"   æ¥æº: {metadata.get('ocr_method', 'unknown')}")
                                        print(f"   é•¿åº¦: {len(text)} å­—ç¬¦")
                                        print(f"\n   å†…å®¹é¢„è§ˆ:")
                                        print(f"   {'-'*76}")
                                        # æ˜¾ç¤ºå‰1000å­—ç¬¦
                                        preview = text[:1000]
                                        for line in preview.split('\n'):
                                            print(f"   {line}")
                                        if len(text) > 1000:
                                            print(
                                                f"   ... (è¿˜æœ‰ {len(text) - 1000} å­—ç¬¦)")
                                        print(f"   {'-'*76}")
                                        break
                                except:
                                    pass
            except Exception as e:
                print(f"âš ï¸  è·å–OCRæ–‡æœ¬å¤±è´¥: {e}")

        if not ocr_found:
            print("\nâš ï¸  æœªæ‰¾åˆ°OCRæ–‡æœ¬")
            print("   å¯èƒ½åŸå› :")
            print("   - OCRæœªå¯ç”¨ï¼ˆOCR_ENABLED=Falseï¼‰")
            print("   - OCRå¤„ç†å¤±è´¥")
            print("   - å‘é‡ç´¢å¼•å°šæœªåˆ›å»º")

        # 5. æ˜¾ç¤ºVision-to-Textå†…å®¹
        print(f"\n{'='*80}")
        print("ğŸ‘ï¸  Vision-to-Textå†…å®¹")
        print("="*80)

        vision_found = False
        if vision_index and vision_index.index_data:
            try:
                index_data = json.loads(vision_index.index_data)
                ctx_ids = index_data.get("context_ids", [])

                if ctx_ids:
                    points = qdrant_client.retrieve(
                        collection_name=collection_name,
                        ids=ctx_ids,
                        with_payload=True,
                    )

                    print(f"\nâœ… æ‰¾åˆ° {len(points)} ä¸ªVision-to-Text chunks\n")

                    for i, point in enumerate(points, 1):
                        if point.payload:
                            node_content = point.payload.get("_node_content")
                            if node_content:
                                try:
                                    payload_data = json.loads(node_content)
                                    metadata = payload_data.get("metadata", {})

                                    # Vision-to-Textçš„æ ‡è¯†ï¼šindex_method == "vision_to_text"
                                    if metadata.get("index_method") == "vision_to_text":
                                        vision_found = True
                                        text = payload_data.get("text", "")
                                        asset_id = metadata.get("asset_id", "")

                                        print(f"{'='*80}")
                                        print(f"Chunk {i}/{len(points)}")
                                        print(f"{'='*80}")
                                        print(f"Asset ID: {asset_id}")
                                        print(f"å†…å®¹é•¿åº¦: {len(text)} å­—ç¬¦")
                                        print(f"\nå†…å®¹:")
                                        print(f"{'-'*76}")
                                        # æ˜¾ç¤ºå®Œæ•´å†…å®¹
                                        for line in text.split('\n'):
                                            print(f"   {line}")
                                        print(f"{'-'*76}\n")
                                except Exception as e:
                                    print(f"âš ï¸  è§£æchunk {i}å¤±è´¥: {e}")
            except Exception as e:
                print(f"âš ï¸  è·å–Vision-to-Textå¤±è´¥: {e}")

        if not vision_found:
            print("\nâš ï¸  æœªæ‰¾åˆ°Vision-to-Textå†…å®¹")
            print("   å¯èƒ½åŸå› :")
            print("   - Visionç´¢å¼•å°šæœªåˆ›å»º")
            print("   - Visionç´¢å¼•åˆ›å»ºå¤±è´¥")
            print("   - Visionæ¨¡å‹æœªé…ç½®")

        # 6. æ˜¾ç¤ºåˆå¹¶åçš„å†…å®¹ç»“æ„
        print(f"\n{'='*80}")
        print("ğŸ”— åˆå¹¶åçš„å†…å®¹ç»“æ„")
        print("="*80)

        if ocr_found and vision_found:
            print("\nâœ… å®Œæ•´åˆå¹¶å†…å®¹ç»“æ„:")
            print("""
------ OCR Text ------
[OCRæå–çš„åŸå§‹æ–‡æœ¬å†…å®¹]

------ Vision Analysis (Asset: file.png) ------
[Vision-to-Textç”Ÿæˆçš„è¯¦ç»†åˆ†æå†…å®¹]
            """)
            print("ğŸ’¡ è¯´æ˜:")
            print("   - OCRæ–‡æœ¬æä¾›åŸå§‹çš„æ–‡å­—è¯†åˆ«ç»“æœ")
            print("   - Vision-to-Textæä¾›ç»“æ„åŒ–çš„æ·±åº¦åˆ†æ")
            print("   - ä¸¤è€…åˆå¹¶åç”¨äºçŸ¥è¯†å›¾è°±æ„å»º")
        elif vision_found:
            print("\nâœ… å½“å‰å†…å®¹ç»“æ„ï¼ˆä»…Vision-to-Textï¼‰:")
            print("""
------ Vision Analysis (Asset: file.png) ------
[Vision-to-Textç”Ÿæˆçš„è¯¦ç»†åˆ†æå†…å®¹]
            """)
            print("ğŸ’¡ è¯´æ˜:")
            print("   - OCRæœªå¯ç”¨ï¼Œä»…ä½¿ç”¨Vision-to-Textå†…å®¹")
            print("   - Vision-to-Textå†…å®¹ç”¨äºçŸ¥è¯†å›¾è°±æ„å»º")
        elif ocr_found:
            print("\nâš ï¸  ä»…OCRæ–‡æœ¬å¯ç”¨:")
            print("   - Visionç´¢å¼•å°šæœªåˆ›å»ºæˆ–å¤±è´¥")
            print("   - å»ºè®®ç­‰å¾…Visionç´¢å¼•åˆ›å»ºå®Œæˆ")
        else:
            print("\nâŒ æ— å¯ç”¨å†…å®¹")
            print("   - è¯·æ£€æŸ¥ç´¢å¼•åˆ›å»ºçŠ¶æ€")

        # 7. æ˜¾ç¤ºçŸ¥è¯†å›¾è°±æ‘˜è¦
        graph_index = session.execute(
            select(DocumentIndex).where(
                and_(
                    DocumentIndex.document_id == document_id,
                    DocumentIndex.index_type == DocumentIndexType.GRAPH
                )
            )
        ).scalar_one_or_none()

        if graph_index and graph_index.index_data:
            try:
                index_data = json.loads(graph_index.index_data)
                entities = index_data.get("entities_extracted", 0)
                relations = index_data.get("relations_extracted", 0)
                chunks = index_data.get("chunks_created", 0)

                print(f"\n{'='*80}")
                print("ğŸ•¸ï¸  çŸ¥è¯†å›¾è°±æ‘˜è¦ï¼ˆåŸºäºåˆå¹¶å†…å®¹æ„å»ºï¼‰")
                print("="*80)
                print(f"\n   æ–‡æœ¬å—æ•°é‡: {chunks}")
                print(f"   å®ä½“æ•°é‡: {entities}")
                print(f"   å…³ç³»æ•°é‡: {relations}")

                if entities > 0 or relations > 0:
                    print(f"\n   âœ… çŸ¥è¯†å›¾è°±å·²æˆåŠŸä»åˆå¹¶å†…å®¹ä¸­æå–å®ä½“å’Œå…³ç³»")
                else:
                    print(f"\n   âš ï¸  çŸ¥è¯†å›¾è°±ä¸­æœªæå–åˆ°å®ä½“å’Œå…³ç³»")
            except:
                pass

        break


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="æŸ¥çœ‹OCRæ–‡æœ¬å’ŒVision-to-Textåˆå¹¶åçš„å†…å®¹"
    )
    parser.add_argument(
        "--document-id",
        type=str,
        required=True,
        help="æ–‡æ¡£ID"
    )

    args = parser.parse_args()

    try:
        view_merged_content(args.document_id)
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

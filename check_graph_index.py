#!/usr/bin/env python3
"""
è¯Šæ–­è„šæœ¬ï¼šæ£€æŸ¥ç‰¹å®šæ–‡æ¡£çš„çŸ¥è¯†å›¾è°±ç´¢å¼•çŠ¶æ€
"""

import sys
import os

# Add the project root to the path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

try:
    from sqlalchemy import select, and_, desc
    from aperag.db.models import Document, DocumentIndex, DocumentIndexType, DocumentIndexStatus, Collection
    from aperag.config import get_sync_session
    from aperag.schema.utils import parseCollectionConfig
    import json
except ImportError as e:
    print(f"å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿åœ¨æ­£ç¡®çš„ç¯å¢ƒä¸­è¿è¡Œæ­¤è„šæœ¬")
    sys.exit(1)


def check_document_graph_index(document_name_pattern: str):
    """æ£€æŸ¥æ–‡æ¡£çš„çŸ¥è¯†å›¾è°±ç´¢å¼•çŠ¶æ€"""

    print("=" * 80)
    print("æ–‡æ¡£çŸ¥è¯†å›¾è°±ç´¢å¼•è¯Šæ–­å·¥å…·")
    print("=" * 80)
    print(f"\næŸ¥æ‰¾æ–‡æ¡£: {document_name_pattern}\n")

    for session in get_sync_session():
        # æŸ¥è¯¢æ–‡æ¡£
        doc_stmt = select(Document).where(
            Document.name.like(f"%{document_name_pattern}%")
        ).order_by(desc(Document.gmt_created))

        doc_result = session.execute(doc_stmt)
        documents = doc_result.scalars().all()

        if not documents:
            print(f"âŒ æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡æ¡£: {document_name_pattern}")
            return

        for doc in documents:
            print(f"\n{'='*80}")
            print(f"æ–‡æ¡£åç§°: {doc.name}")
            print(f"æ–‡æ¡£ID: {doc.id}")
            print(f"æ–‡æ¡£çŠ¶æ€: {doc.status}")
            print(f"æ‰€å±Collection ID: {doc.collection_id}")
            print(f"æ–‡ä»¶å¤§å°: {doc.size} bytes")
            print(f"åˆ›å»ºæ—¶é—´: {doc.gmt_created}")
            print(f"æ›´æ–°æ—¶é—´: {doc.gmt_updated}")
            print(f"-" * 80)

            # æŸ¥è¯¢Collectioné…ç½®
            collection_stmt = select(Collection).where(
                Collection.id == doc.collection_id)
            collection_result = session.execute(collection_stmt)
            collection = collection_result.scalar_one_or_none()

            if collection:
                print(f"\nğŸ“š Collectionä¿¡æ¯:")
                print(f"  Collectionåç§°: {collection.name}")
                print(f"  CollectionçŠ¶æ€: {collection.status}")

                # æ£€æŸ¥çŸ¥è¯†å›¾è°±æ˜¯å¦å¯ç”¨
                try:
                    config = parseCollectionConfig(collection.config)
                    enable_kg = config.enable_knowledge_graph if hasattr(
                        config, 'enable_knowledge_graph') else False
                    print(f"  çŸ¥è¯†å›¾è°±å¯ç”¨çŠ¶æ€: {'âœ… å·²å¯ç”¨' if enable_kg else 'âŒ æœªå¯ç”¨'}")

                    if hasattr(config, 'knowledge_graph_config'):
                        kg_config = config.knowledge_graph_config
                        print(f"  çŸ¥è¯†å›¾è°±é…ç½®:")
                        if hasattr(kg_config, 'language'):
                            print(f"    - è¯­è¨€: {kg_config.language}")
                        if hasattr(kg_config, 'entity_types'):
                            print(f"    - å®ä½“ç±»å‹: {kg_config.entity_types}")
                except Exception as e:
                    print(f"  âš ï¸  è§£æCollectioné…ç½®å¤±è´¥: {e}")
                    print(f"  åŸå§‹é…ç½®: {collection.config}")
            else:
                print(f"\nâš ï¸  æœªæ‰¾åˆ°Collection: {doc.collection_id}")

            # æŸ¥è¯¢æ‰€æœ‰ç´¢å¼•çŠ¶æ€
            index_stmt = select(DocumentIndex).where(
                DocumentIndex.document_id == doc.id
            )
            index_result = session.execute(index_stmt)
            indexes = index_result.scalars().all()

            if not indexes:
                print("\nâš ï¸  æœªæ‰¾åˆ°ä»»ä½•ç´¢å¼•è®°å½•")
                continue

            print("\nğŸ“Š ç´¢å¼•çŠ¶æ€è¯¦æƒ…:")
            graph_index_found = False

            for idx in indexes:
                status_icon = {
                    DocumentIndexStatus.PENDING: "â³",
                    DocumentIndexStatus.CREATING: "ğŸ”„",
                    DocumentIndexStatus.COMPLETED: "âœ…",
                    DocumentIndexStatus.FAILED: "âŒ",
                    DocumentIndexStatus.DELETION_IN_PROGRESS: "ğŸ—‘ï¸",
                    DocumentIndexStatus.SKIPPED: "â­ï¸"
                }.get(idx.status, "â“")

                print(f"\n  {status_icon} {idx.index_type.value} ç´¢å¼•:")
                print(f"     çŠ¶æ€: {idx.status.value}")
                print(f"     ç‰ˆæœ¬: {idx.version}")
                print(f"     åˆ›å»ºæ—¶é—´: {idx.gmt_created}")
                print(f"     æ›´æ–°æ—¶é—´: {idx.gmt_updated}")

                if idx.index_type == DocumentIndexType.GRAPH:
                    graph_index_found = True
                    if idx.error_message:
                        print(f"     âŒ é”™è¯¯ä¿¡æ¯:")
                        print(f"        {idx.error_message}")

                    if idx.index_data:
                        try:
                            data = json.loads(idx.index_data)
                            print(f"     ğŸ“Š çŸ¥è¯†å›¾è°±æ•°æ®æ‘˜è¦:")
                            if "chunks_created" in data:
                                print(
                                    f"        - å—æ•°é‡: {data['chunks_created']}")
                            if "entities_extracted" in data:
                                print(
                                    f"        - å®ä½“æ•°é‡: {data['entities_extracted']}")
                            if "relations_extracted" in data:
                                print(
                                    f"        - å…³ç³»æ•°é‡: {data['relations_extracted']}")
                            if "status" in data:
                                print(f"        - å¤„ç†çŠ¶æ€: {data['status']}")
                        except:
                            print(
                                f"     ğŸ“Š ç´¢å¼•æ•°æ®: {str(idx.index_data)[:200]}...")

            # è¯Šæ–­å»ºè®®
            print(f"\n{'='*80}")
            print("ğŸ” è¯Šæ–­ç»“æœ:")
            print("="*80)

            if not collection:
                print("\nâŒ Collectionä¸å­˜åœ¨ï¼Œæ— æ³•åˆ›å»ºçŸ¥è¯†å›¾è°±")
            elif not enable_kg:
                print("\nâŒ çŸ¥è¯†å›¾è°±æœªå¯ç”¨")
                print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
                print("   1. é€šè¿‡Webç•Œé¢è¿›å…¥Collectionè®¾ç½®")
                print("   2. å¯ç”¨'çŸ¥è¯†å›¾è°±'é€‰é¡¹")
                print("   3. é‡æ–°ä¸Šä¼ æ–‡æ¡£æˆ–é‡å»ºç´¢å¼•")
            elif not graph_index_found:
                print("\nâŒ æœªæ‰¾åˆ°GRAPHç´¢å¼•è®°å½•")
                print("\nğŸ’¡ å¯èƒ½åŸå› :")
                print("   - ç´¢å¼•åˆ›å»ºä»»åŠ¡å°šæœªæ‰§è¡Œ")
                print("   - ç´¢å¼•åˆ›å»ºå¤±è´¥ä½†æœªè®°å½•é”™è¯¯")
                print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
                print("   1. æ£€æŸ¥celery workeræ—¥å¿—")
                print("   2. é€šè¿‡APIé‡å»ºGRAPHç´¢å¼•")
            else:
                graph_idx = next(
                    (idx for idx in indexes if idx.index_type == DocumentIndexType.GRAPH), None)
                if graph_idx:
                    if graph_idx.status == DocumentIndexStatus.FAILED:
                        print(f"\nâŒ çŸ¥è¯†å›¾è°±ç´¢å¼•åˆ›å»ºå¤±è´¥")
                        print(f"   é”™è¯¯: {graph_idx.error_message}")
                        print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
                        print("   1. æ£€æŸ¥LLMæœåŠ¡é…ç½®ï¼ˆç”¨äºå®ä½“å’Œå…³ç³»æå–ï¼‰")
                        print("   2. æ£€æŸ¥å›¾æ•°æ®åº“è¿æ¥ï¼ˆå¦‚æœä½¿ç”¨Neo4j/NebulaGraphï¼‰")
                        print("   3. æŸ¥çœ‹celery workeræ—¥å¿—è·å–è¯¦ç»†é”™è¯¯")
                        print("   4. é€šè¿‡APIé‡å»ºGRAPHç´¢å¼•")
                    elif graph_idx.status == DocumentIndexStatus.PENDING:
                        print(f"\nâ³ çŸ¥è¯†å›¾è°±ç´¢å¼•ç­‰å¾…å¤„ç†ä¸­")
                        print("\nğŸ’¡ è¯´æ˜:")
                        print("   - ç´¢å¼•ä»»åŠ¡å·²åˆ›å»ºï¼Œç­‰å¾…celery workerå¤„ç†")
                        print("   - é€šå¸¸ä¼šåœ¨30ç§’å†…å¼€å§‹å¤„ç†")
                    elif graph_idx.status == DocumentIndexStatus.CREATING:
                        print(f"\nğŸ”„ çŸ¥è¯†å›¾è°±ç´¢å¼•æ­£åœ¨åˆ›å»ºä¸­")
                        print("\nğŸ’¡ è¯´æ˜:")
                        print("   - ç´¢å¼•æ­£åœ¨å¤„ç†ï¼Œè¯·è€å¿ƒç­‰å¾…")
                        print("   - å¤§æ–‡æ¡£å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´")
                    elif graph_idx.status == DocumentIndexStatus.COMPLETED:
                        print(f"\nâœ… çŸ¥è¯†å›¾è°±ç´¢å¼•å·²æˆåŠŸåˆ›å»º")
                        print("\nğŸ’¡ å¦‚æœä»çœ‹ä¸åˆ°çŸ¥è¯†å›¾è°±:")
                        print("   1. æ£€æŸ¥å‰ç«¯æ˜¯å¦æ­£ç¡®åŠ è½½")
                        print("   2. æ£€æŸ¥çŸ¥è¯†å›¾è°±æŸ¥è¯¢API")
                        print("   3. æŸ¥çœ‹æ˜¯å¦æœ‰å®ä½“å’Œå…³ç³»æ•°æ®")
                    elif graph_idx.status == DocumentIndexStatus.SKIPPED:
                        print(f"\nâ­ï¸  çŸ¥è¯†å›¾è°±ç´¢å¼•è¢«è·³è¿‡")
                        print(f"   åŸå› : {graph_idx.error_message or 'æœªçŸ¥'}")

            print(f"\n{'='*80}\n")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="æ£€æŸ¥æ–‡æ¡£çš„çŸ¥è¯†å›¾è°±ç´¢å¼•çŠ¶æ€")
    parser.add_argument(
        "document_name",
        nargs="?",
        default="B5391S-T0102-åœŸå»ºæ€»å¹³é¢å¸ƒç½®å›¾",
        help="æ–‡æ¡£åç§°æˆ–åç§°çš„ä¸€éƒ¨åˆ†(æ”¯æŒæ¨¡ç³ŠåŒ¹é…)"
    )

    args = parser.parse_args()

    try:
        check_document_graph_index(args.document_name)
    except Exception as e:
        print(f"\nâŒ è¯Šæ–­è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

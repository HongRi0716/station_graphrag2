#!/usr/bin/env python3
"""
è¯Šæ–­è„šæœ¬:æŸ¥çœ‹æ–‡æ¡£ç´¢å¼•å¤±è´¥çš„è¯¦ç»†é”™è¯¯ä¿¡æ¯
"""

from sqlalchemy import select, and_, desc
from aperag.db.models import Document, DocumentIndex, DocumentIndexType, DocumentIndexStatus
from aperag.config import get_sync_session
import os
import sys

# Add the project root to the path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))


def diagnose_document_failure(document_name_pattern: str = None):
    """è¯Šæ–­æ–‡æ¡£ç´¢å¼•å¤±è´¥çš„åŸå› """

    print("=" * 80)
    print("æ–‡æ¡£ç´¢å¼•å¤±è´¥è¯Šæ–­å·¥å…·")
    print("=" * 80)

    for session in get_sync_session():
        # æŸ¥è¯¢æ–‡æ¡£
        if document_name_pattern:
            doc_stmt = select(Document).where(
                Document.name.like(f"%{document_name_pattern}%")
            ).order_by(desc(Document.gmt_created))
        else:
            doc_stmt = select(Document).order_by(
                desc(Document.gmt_created)).limit(10)

        doc_result = session.execute(doc_stmt)
        documents = doc_result.scalars().all()

        if not documents:
            print(f"\næœªæ‰¾åˆ°åŒ¹é…çš„æ–‡æ¡£: {document_name_pattern}")
            return

        print(f"\næ‰¾åˆ° {len(documents)} ä¸ªåŒ¹é…çš„æ–‡æ¡£:\n")

        for doc in documents:
            print(f"\n{'='*80}")
            print(f"æ–‡æ¡£åç§°: {doc.name}")
            print(f"æ–‡æ¡£ID: {doc.id}")
            print(f"æ–‡æ¡£çŠ¶æ€: {doc.status}")
            print(f"æ‰€å±Collection: {doc.collection_id}")
            print(f"æ–‡ä»¶å¤§å°: {doc.size} bytes")
            print(f"åˆ›å»ºæ—¶é—´: {doc.gmt_created}")
            print(f"æ›´æ–°æ—¶é—´: {doc.gmt_updated}")
            print(f"-" * 80)

            # æŸ¥è¯¢æ‰€æœ‰ç´¢å¼•çŠ¶æ€
            index_stmt = select(DocumentIndex).where(
                DocumentIndex.document_id == doc.id
            )
            index_result = session.execute(index_stmt)
            indexes = index_result.scalars().all()

            if not indexes:
                print("  âš ï¸  æœªæ‰¾åˆ°ä»»ä½•ç´¢å¼•è®°å½•")
                continue

            print("\nç´¢å¼•çŠ¶æ€è¯¦æƒ…:")
            for idx in indexes:
                status_icon = {
                    DocumentIndexStatus.PENDING: "â³",
                    DocumentIndexStatus.CREATING: "ğŸ”„",
                    DocumentIndexStatus.COMPLETED: "âœ…",
                    DocumentIndexStatus.FAILED: "âŒ",
                    DocumentIndexStatus.DELETION_IN_PROGRESS: "ğŸ—‘ï¸",
                    DocumentIndexStatus.SKIPPED: "â­ï¸"
                }.get(idx.status, "â“")

                print(f"\n  {status_icon} {idx.index_type.value} ç´¢å¼•:")
                print(f"     çŠ¶æ€: {idx.status.value}")
                print(f"     ç‰ˆæœ¬: {idx.version}")
                print(f"     åˆ›å»ºæ—¶é—´: {idx.gmt_created}")
                print(f"     æ›´æ–°æ—¶é—´: {idx.gmt_updated}")

                if idx.error_message:
                    print(f"     âŒ é”™è¯¯ä¿¡æ¯:")
                    print(f"        {idx.error_message}")

                if idx.index_data:
                    import json
                    try:
                        data = json.loads(idx.index_data)
                        print(f"     ğŸ“Š ç´¢å¼•æ•°æ®æ‘˜è¦:")
                        if "context_ids" in data:
                            print(
                                f"        - å‘é‡æ•°é‡: {len(data['context_ids'])}")
                        if "chunk_count" in data:
                            print(f"        - å—æ•°é‡: {data['chunk_count']}")
                        if "entity_count" in data:
                            print(f"        - å®ä½“æ•°é‡: {data['entity_count']}")
                        if "relationship_count" in data:
                            print(
                                f"        - å…³ç³»æ•°é‡: {data['relationship_count']}")
                    except:
                        pass

            print(f"\n{'='*80}\n")

        # æä¾›è¯Šæ–­å»ºè®®
        print("\n" + "="*80)
        print("è¯Šæ–­å»ºè®®:")
        print("="*80)

        failed_indexes = [idx for doc in documents for idx in session.execute(
            select(DocumentIndex).where(
                and_(
                    DocumentIndex.document_id == doc.id,
                    DocumentIndex.status == DocumentIndexStatus.FAILED
                )
            )
        ).scalars().all()]

        if failed_indexes:
            print("\nå‘ç°å¤±è´¥çš„ç´¢å¼•:")
            for idx in failed_indexes:
                print(f"\n  ç±»å‹: {idx.index_type.value}")
                print(f"  é”™è¯¯: {idx.error_message}")

                # æ ¹æ®é”™è¯¯ç±»å‹æä¾›å»ºè®®
                if "embedding" in idx.error_message.lower():
                    print("\n  ğŸ’¡ å¯èƒ½åŸå› : å‘é‡åµŒå…¥æœåŠ¡é—®é¢˜")
                    print("     - æ£€æŸ¥ EMBEDDING_PROVIDER å’Œç›¸å…³é…ç½®")
                    print("     - æ£€æŸ¥å‘é‡æœåŠ¡(å¦‚OpenAI API)æ˜¯å¦å¯è®¿é—®")
                    print("     - æŸ¥çœ‹ envs/.env ä¸­çš„åµŒå…¥æœåŠ¡é…ç½®")

                elif "graph" in idx.error_message.lower() or "knowledge" in idx.error_message.lower():
                    print("\n  ğŸ’¡ å¯èƒ½åŸå› : çŸ¥è¯†å›¾è°±æ„å»ºé—®é¢˜")
                    print("     - æ£€æŸ¥ Neo4j æˆ– NebulaGraph è¿æ¥é…ç½®")
                    print("     - æ£€æŸ¥ LLM æœåŠ¡æ˜¯å¦æ­£å¸¸(ç”¨äºå®ä½“å’Œå…³ç³»æå–)")
                    print("     - æŸ¥çœ‹å›¾æ•°æ®åº“æ˜¯å¦è¿è¡Œæ­£å¸¸")

                elif "qdrant" in idx.error_message.lower() or "vector" in idx.error_message.lower():
                    print("\n  ğŸ’¡ å¯èƒ½åŸå› : å‘é‡æ•°æ®åº“è¿æ¥é—®é¢˜")
                    print("     - æ£€æŸ¥ Qdrant æœåŠ¡æ˜¯å¦è¿è¡Œ")
                    print("     - æ£€æŸ¥ VECTOR_DB_CONTEXT é…ç½®")

                elif "llm" in idx.error_message.lower() or "api" in idx.error_message.lower():
                    print("\n  ğŸ’¡ å¯èƒ½åŸå› : LLM API è°ƒç”¨å¤±è´¥")
                    print("     - æ£€æŸ¥ LLM API Key æ˜¯å¦æœ‰æ•ˆ")
                    print("     - æ£€æŸ¥ API é…é¢æ˜¯å¦å……è¶³")
                    print("     - æ£€æŸ¥ç½‘ç»œè¿æ¥")

                elif "parse" in idx.error_message.lower() or "docx" in idx.error_message.lower():
                    print("\n  ğŸ’¡ å¯èƒ½åŸå› : æ–‡æ¡£è§£æé—®é¢˜")
                    print("     - æ–‡æ¡£å¯èƒ½æŸåæˆ–æ ¼å¼ä¸æ”¯æŒ")
                    print("     - æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å¯ä»¥æ­£å¸¸æ‰“å¼€")
                    print("     - å°è¯•é‡æ–°ä¸Šä¼ æ–‡æ¡£")

                print("\n  ğŸ”§ ä¿®å¤å»ºè®®:")
                print("     1. ä¿®å¤ä¸Šè¿°é…ç½®é—®é¢˜")
                print("     2. ä½¿ç”¨ rebuild_document_indexes API é‡å»ºå¤±è´¥çš„ç´¢å¼•")
                print(
                    f"        POST /api/v1/collections/{{collection_id}}/documents/{idx.document_id}/rebuild-indexes")
                print(
                    f"        Body: {{\"index_types\": [\"{idx.index_type.value}\"]}}")
        else:
            print("\nâœ… æœªå‘ç°å¤±è´¥çš„ç´¢å¼•")

        print("\n" + "="*80)
        print("æ›´å¤šä¿¡æ¯:")
        print("  - æŸ¥çœ‹ Celery worker æ—¥å¿—ä»¥è·å–è¯¦ç»†é”™è¯¯å †æ ˆ")
        print("  - æ£€æŸ¥ envs/.env é…ç½®æ–‡ä»¶")
        print("  - è¿è¡Œ: docker-compose logs celery-worker")
        print("="*80 + "\n")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="è¯Šæ–­æ–‡æ¡£ç´¢å¼•å¤±è´¥åŸå› ")
    parser.add_argument(
        "document_name",
        nargs="?",
        default="å˜ç”µç«™å›¾çº¸æ¡£æ¡ˆæ™ºèƒ½åŒ–ç®¡ç†æŠ€æœ¯",
        help="æ–‡æ¡£åç§°æˆ–åç§°çš„ä¸€éƒ¨åˆ†(æ”¯æŒæ¨¡ç³ŠåŒ¹é…)"
    )

    args = parser.parse_args()

    try:
        diagnose_document_failure(args.document_name)
    except Exception as e:
        print(f"\nâŒ è¯Šæ–­è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
